\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baltrunas2011incarmusic}
\citation{nickel2011three}
\citation{wang2019three}
\citation{zhou2013tensor}
\citation{hong2018generalized,wang2019multiway}
\citation{ghadermarzy2018learning,montanari2018spectral}
\citation{hong2018generalized,wang2017tensor}
\citation{hitchcock1927expression}
\citation{tucker1966some}
\citation{kolda2009tensor,ghadermarzy2019near}
\citation{ghadermarzy2018learning}
\citation{hong2018generalized,ghadermarzy2018learning}
\citation{mccullagh1980regression}
\citation{ghadermarzy2018learning}
\citation{hillar2013most}
\citation{ghadermarzy2018learning}
\citation{cai2013max,davenport2014,bhaskar20151}
\citation{bhaskar2016probabilistic}
\newlabel{tab:compare}{{1}{2}{Comparison with previous work. For ease of presentation, we summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode}{table.1}{}}
\newlabel{eq:model}{{1}{2}{}{equation.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{eq:logodd}{{2}{2}{}{equation.3.2}{}}
\citation{mccullagh1980regression}
\citation{zhou2013tensor,bhaskar20151}
\citation{kolda2009tensor}
\citation{hitchcock1927expression}
\citation{oseledets2011tensor}
\citation{de2008tensor}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\newlabel{sec:latent}{{3.2}{3}{}{subsection.3.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:quantization}{{3}{3}{}{equation.3.3}{}}
\newlabel{eq:latent}{{4}{3}{}{equation.3.4}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\newlabel{ass:link}{{1}{3}{}{assumption.1}{}}
\newlabel{sec:denoising}{{3.3}{3}{}{subsection.3.3}{}}
\newlabel{eq:space}{{5}{3}{}{equation.3.5}{}}
\newlabel{eq:Tucker}{{6}{3}{}{equation.3.6}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:model}
\citation{negahban2011estimation,cai2013max,bhaskar20151}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\MT@newlabel{eq:space}
\newlabel{sec:theory}{{4}{4}{}{section.4}{}}
\newlabel{eq:objective}{{7}{4}{}{equation.4.7}{}}
\newlabel{eq:estimator}{{8}{4}{}{equation.4.8}{}}
\newlabel{eq:regular}{{4}{4}{}{equation.4.8}{}}
\MT@newlabel{eq:latent}
\newlabel{sec:denosing}{{4.1}{4}{}{subsection.4.1}{}}
\MT@newlabel{eq:estimator}
\newlabel{thm:rate}{{4.1}{4}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:estimator}
\newlabel{eq:rate}{{9}{4}{Statistical convergence}{equation.4.9}{}}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:estimator}
\newlabel{cor:prediction}{{1}{4}{Prediction error}{cor.1}{}}
\newlabel{eq:KLrate}{{10}{4}{Prediction error}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\newlabel{eq:ours}{{4.1}{4}{}{equation.4.10}{}}
\citation{bhaskar2016probabilistic}
\citation{mu2014square}
\citation{yuan2016tensor}
\citation{zhang2019cross}
\citation{ghadermarzy2018learning}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:KLrate}
\MT@newlabel{eq:estimator}
\newlabel{thm:minimax}{{4.2}{5}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:lower}{{4.2}{5}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:estimator}
\newlabel{eq:weighted}{{11}{5}{}{equation.4.11}{}}
\MT@newlabel{eq:weighted}
\newlabel{thm:completion}{{4.3}{5}{}{thm.4.3}{}}
\MT@newlabel{eq:estimator}
\newlabel{sec:algorithm}{{5}{5}{}{section.5}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:Tucker}
\MT@newlabel{eq:objective}
\citation{davenport2014,sur2019modern}
\newlabel{alg}{{1}{6}{}{algorithm.1}{}}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:model}
\newlabel{fig:stability}{{5}{6}{}{ALC@unique.13}{}}
\newlabel{eq:BIC}{{5}{6}{}{figure.1}{}}
\newlabel{sec:experiment}{{6}{6}{}{section.6}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:Tucker}
\newlabel{sec:simulation}{{6.1}{6}{}{subsection.6.1}{}}
\MT@newlabel{eq:rate}
\citation{acar2010scalable}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\newlabel{fig:finite}{{2}{7}{Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\FnormSize {}{\hat \Theta -\trueT }/\FnormSize {}{\trueT }$ for better visualization}{figure.2}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\citation{geddes2016human}
\citation{baltrunas2011incarmusic}
\citation{baltrunas2011incarmusic}
\newlabel{fig:compare}{{3}{8}{Performance comparison in MCR (a, b) and MAD (c, d). (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. (a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$}{figure.3}{}}
\MT@newlabel{eq:latent}
\newlabel{table:CV}{{2}{8}{Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses}{table.2}{}}
\bibdata{tensor_wang}
\bibcite{acar2010scalable}{{1}{2010}{{Acar et~al.}}{{Acar, Dunlavy, Kolda, and M{\o }rup}}}
\bibcite{baltrunas2011incarmusic}{{2}{2011}{{Baltrunas et~al.}}{{Baltrunas, Kaminskas, Ludwig, Moling, Ricci, Aydin, L{\"u}ke, and Schwaiger}}}
\bibcite{bhaskar2016probabilistic}{{3}{2016}{{Bhaskar}}{{}}}
\bibcite{bhaskar20151}{{4}{2015}{{Bhaskar \& Javanmard}}{{Bhaskar and Javanmard}}}
\bibcite{brascamp2002extensions}{{5}{2002}{{Brascamp \& Lieb}}{{Brascamp and Lieb}}}
\bibcite{cai2013max}{{6}{2013}{{Cai \& Zhou}}{{Cai and Zhou}}}
\bibcite{davenport2014}{{7}{2014}{{Davenport et~al.}}{{Davenport, Plan, Van Den~Berg, and Wootters}}}
\bibcite{de2008tensor}{{8}{2008}{{De~Silva \& Lim}}{{De~Silva and Lim}}}
\bibcite{friedland2018nuclear}{{9}{2018}{{Friedland \& Lim}}{{Friedland and Lim}}}
\bibcite{geddes2016human}{{10}{2016}{{Geddes}}{{}}}
\bibcite{ghadermarzy2018learning}{{11}{2018}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{ghadermarzy2019near}{{12}{2019}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{hillar2013most}{{13}{2013}{{Hillar \& Lim}}{{Hillar and Lim}}}
\bibcite{hitchcock1927expression}{{14}{1927}{{Hitchcock}}{{}}}
\bibcite{hong2018generalized}{{15}{2019}{{Hong et~al.}}{{Hong, Kolda, and Duersch}}}
\bibcite{jiang2017tensor}{{16}{2017}{{Jiang et~al.}}{{Jiang, Yang, and Zhang}}}
\bibcite{kolda2009tensor}{{17}{2009}{{Kolda \& Bader}}{{Kolda and Bader}}}
\bibcite{lim2005singular}{{18}{2005}{{Lim}}{{}}}
\bibcite{mccullagh1980regression}{{19}{1980}{{McCullagh}}{{}}}
\bibcite{montanari2018spectral}{{20}{2018}{{Montanari \& Sun}}{{Montanari and Sun}}}
\bibcite{mu2014square}{{21}{2014}{{Mu et~al.}}{{Mu, Huang, Wright, and Goldfarb}}}
\bibcite{negahban2011estimation}{{22}{2011}{{Negahban et~al.}}{{Negahban, Wainwright, et~al.}}}
\bibcite{nguyen2015tensor}{{23}{2015}{{Nguyen et~al.}}{{Nguyen, Drineas, and Tran}}}
\bibcite{nickel2011three}{{24}{2011}{{Nickel et~al.}}{{Nickel, Tresp, and Kriegel}}}
\bibcite{oseledets2011tensor}{{25}{2011}{{Oseledets}}{{}}}
\bibcite{stoeckel2009supramarginal}{{26}{2009}{{Stoeckel et~al.}}{{Stoeckel, Gough, Watkins, and Devlin}}}
\bibcite{sur2019modern}{{27}{2019}{{Sur \& Cand{\`e}s}}{{Sur and Cand{\`e}s}}}
\bibcite{tomioka2014spectral}{{28}{2014}{{Tomioka \& Suzuki}}{{Tomioka and Suzuki}}}
\bibcite{tucker1966some}{{29}{1966}{{Tucker}}{{}}}
\bibcite{wang2017tensor}{{30}{2017}{{Wang \& Song}}{{Wang and Song}}}
\bibcite{wang2019multiway}{{31}{2019}{{Wang \& Zeng}}{{Wang and Zeng}}}
\bibcite{wang2017operator}{{32}{2017}{{Wang et~al.}}{{Wang, Duc, Fischer, and Song}}}
\bibcite{wang2019three}{{33}{2019}{{Wang et~al.}}{{Wang, Fischer, Song, et~al.}}}
\bibcite{yuan2016tensor}{{34}{2016}{{Yuan \& Zhang}}{{Yuan and Zhang}}}
\bibcite{zhang2019cross}{{35}{2019}{{Zhang}}{{}}}
\bibcite{zhou2013tensor}{{36}{2013}{{Zhou et~al.}}{{Zhou, Li, and Zhu}}}
\bibstyle{icml2020}
\newlabel{sec:proof}{{A}{11}{Acknowledgements}{appendix.A}{}}
\newlabel{sec:proofMSE}{{A.1}{11}{Acknowledgements}{subsection.A.1}{}}
\newlabel{eq:property}{{12}{11}{Acknowledgements}{equation.A.12}{}}
\MT@newlabel{eq:property}
\newlabel{eq:bound}{{A.1}{11}{Acknowledgements}{equation.A.12}{}}
\newlabel{eq:taylor}{{13}{11}{Acknowledgements}{equation.A.13}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:linear}{{14}{11}{Acknowledgements}{equation.A.14}{}}
\MT@newlabel{eq:property}
\newlabel{eq:norm}{{15}{11}{Acknowledgements}{equation.A.15}{}}
\newlabel{eq:normrandom}{{16}{11}{Acknowledgements}{equation.A.16}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:norm}
\MT@newlabel{eq:normrandom}
\newlabel{eq:linearconclusion}{{17}{11}{Acknowledgements}{equation.A.17}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:quadratic}{{18}{12}{Acknowledgements}{equation.A.18}{}}
\MT@newlabel{eq:taylor}
\MT@newlabel{eq:linearconclusion}
\MT@newlabel{eq:quadratic}
\newlabel{eq:KLbound1}{{19}{12}{Acknowledgements}{equation.A.19}{}}
\MT@newlabel{eq:KLbound1}
\newlabel{eq:KLbound}{{20}{12}{Acknowledgements}{equation.A.20}{}}
\MT@newlabel{eq:KLbound}
\newlabel{eq:totalKL}{{21}{12}{Acknowledgements}{equation.A.21}{}}
\MT@newlabel{eq:totalKL}
\newlabel{eq:final}{{22}{12}{Acknowledgements}{equation.A.22}{}}
\MT@newlabel{eq:final}
\newlabel{eq:prob}{{A.1}{13}{Acknowledgements}{equation.A.22}{}}
\newlabel{eq:Taylor2}{{23}{13}{Acknowledgements}{equation.A.23}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:quadratic}
\newlabel{eq:linear2}{{A.2}{13}{Acknowledgements}{Item.4}{}}
\newlabel{eq:quadratic2}{{24}{13}{Acknowledgements}{equation.A.24}{}}
\MT@newlabel{eq:Taylor2}
\MT@newlabel{eq:quadratic2}
\newlabel{eq:sample}{{25}{13}{Acknowledgements}{equation.A.25}{}}
\MT@newlabel{eq:sample}
\citation{brascamp2002extensions}
\citation{ghadermarzy2019near}
\citation{lim2005singular}
\citation{friedland2018nuclear}
\citation{ghadermarzy2019near}
\newlabel{sec:proofconvexity}{{A.3}{14}{Acknowledgements}{subsection.A.3}{}}
\newlabel{thm:convexity}{{A.1}{14}{}{thm.A.1}{}}
\newlabel{eq:function}{{26}{14}{}{equation.A.26}{}}
\MT@newlabel{eq:function}
\newlabel{lem:lossconvexity}{{1}{14}{Corollary 3.5 in~\cite {brascamp2002extensions}}{lem.1}{}}
\newlabel{sec:lemma}{{A.4}{14}{Acknowledgements}{subsection.A.4}{}}
\citation{wang2017operator}
\citation{jiang2017tensor}
\citation{friedland2018nuclear}
\citation{nguyen2015tensor}
\citation{tomioka2014spectral}
\citation{tomioka2014spectral}
\newlabel{lem:Mnormbound}{{2}{15}{M-norm and infinity norm~\citep {ghadermarzy2019near}}{lem.2}{}}
\newlabel{lem:nuclear}{{3}{15}{Nuclear norm and F-norm}{lem.3}{}}
\newlabel{eq:norminequality}{{27}{15}{Acknowledgements}{equation.A.27}{}}
\MT@newlabel{eq:norminequality}
\newlabel{lem:inq}{{4}{15}{}{lem.4}{}}
\newlabel{lem:tensor}{{5}{15}{Spectral norm of random tensors~\citep {tomioka2014spectral}}{lem.5}{}}
\newlabel{lem:noisytensor}{{6}{16}{}{lem.6}{}}
\newlabel{lem:KLentry}{{7}{16}{}{lem.7}{}}
\newlabel{eq:KL}{{A.4}{16}{Acknowledgements}{lem.7}{}}
\newlabel{lem:KL}{{8}{16}{KL divergence and F-norm}{lem.8}{}}
\MT@newlabel{eq:model}
\newlabel{eq:ass}{{28}{16}{KL divergence and F-norm}{equation.A.28}{}}
\MT@newlabel{eq:model}
\newlabel{eq:entrywise}{{29}{17}{Acknowledgements}{equation.A.29}{}}
\MT@newlabel{eq:ass}
\MT@newlabel{eq:entrywise}
\newlabel{lem:construction}{{9}{17}{}{lem.9}{}}
\citation{tsybakov2008introduction}
\citation{ghadermarzy2019near}
\newlabel{lem:VGbound}{{10}{18}{Varshamov-Gilbert bound}{lem.10}{}}
\newlabel{lem:Tsybakov}{{11}{18}{Theorem 2.5 in~\cite {tsybakov2008introduction}}{lem.11}{}}
\newlabel{lem:convexity}{{12}{18}{Lemma 28 in~\cite {ghadermarzy2019near}}{lem.12}{}}
\newlabel{sec:extention}{{B}{19}{Acknowledgements}{appendix.B}{}}
\newlabel{eq:joint}{{30}{19}{Acknowledgements}{equation.B.30}{}}
\newlabel{eq:decomp}{{31}{19}{Acknowledgements}{equation.B.31}{}}
\newlabel{ass:joint}{{2}{19}{}{assumption.2}{}}
\newlabel{eq:constantZ}{{32}{19}{Acknowledgements}{equation.B.32}{}}
\newlabel{thm:ratejoint}{{B.1}{20}{Statistical convergence with unknown $\mb $}{thm.B.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:joint}
\newlabel{eq:scoreZ}{{33}{20}{Statistical convergence with unknown $\mb $}{equation.B.33}{}}
\newlabel{eq:jointTheta}{{B.1}{20}{Statistical convergence with unknown $\mb $}{equation.B.33}{}}
\newlabel{eq:jointb}{{B.1}{20}{Statistical convergence with unknown $\mb $}{equation.B.33}{}}
\newlabel{eq:loglz}{{B}{20}{Acknowledgements}{equation.B.33}{}}
\newlabel{eq:ineql}{{34}{20}{Acknowledgements}{equation.B.34}{}}
\MT@newlabel{eq:ineql}
\newlabel{eq:boundZ}{{35}{20}{Acknowledgements}{equation.B.35}{}}
\MT@newlabel{eq:scoreZ}
\newlabel{eq:scorebound}{{36}{20}{Acknowledgements}{equation.B.36}{}}
\newlabel{eq:HessionZ}{{37}{21}{Acknowledgements}{equation.B.37}{}}
\MT@newlabel{eq:constantZ}
\newlabel{eq:sum}{{38}{21}{Acknowledgements}{equation.B.38}{}}
\newlabel{eq:negativeH}{{39}{22}{Acknowledgements}{equation.B.39}{}}
\MT@newlabel{eq:negativeH}
\MT@newlabel{eq:negativeH}
\MT@newlabel{eq:sum}
\MT@newlabel{eq:negativeH}
\newlabel{eq:boundH}{{2}{22}{Acknowledgements}{equation.B.39}{}}
\MT@newlabel{eq:HessionZ}
\MT@newlabel{eq:scorebound}
\MT@newlabel{eq:HessionZ}
\MT@newlabel{eq:boundZ}
\MT@newlabel{eq:decomp}
\citation{kolda2009tensor}
\MT@newlabel{eq:latent}
\newlabel{fig:compare2}{{4}{23}{Performance comparison for predicting median labels. (a, c) Prediction error versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. (b, d) Prediction error versus the number of ordinal levels $L$, when $\rho =0.8.$}{figure.4}{}}
\newlabel{sec:additionalHCP}{{D}{23}{Acknowledgements}{appendix.D}{}}
\newlabel{eq:Tuckerest}{{40}{23}{Acknowledgements}{equation.D.40}{}}
\citation{stoeckel2009supramarginal}
\MT@newlabel{eq:Tuckerest}
\newlabel{figure:elbow}{{5}{24}{Elbow plot for determining the number of clusters in $K$-means}{figure.5}{}}
\newlabel{figure:brain image}{{6}{24}{Top three clusters in the HCP analysis. (a) Cluster I reflects the connections between two brain hemispheres. (b)-(c) Cluster II/III consists of nodes within left/right hemisphere only. Node name are shown in abbreviation. Edges are colored based on predicted connection level averaged across individuals}{figure.6}{}}
\newlabel{table:clustering}{{3}{25}{Node clusters in the HCP analysis. The first alphabet in the node name indicates the left (L) or right (R) hemisphere. The number in the parentheses indicates the node count in each cluster}{table.3}{}}
