\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2010)Acar, Dunlavy, Kolda, and M{\o}rup]{acar2010scalable}
Evrim Acar, Daniel~M Dunlavy, Tamara~G Kolda, and Morten M{\o}rup.
\newblock Scalable tensor factorizations with missing data.
\newblock In \emph{Proceedings of the 2010 SIAM international conference on
  data mining}, pages 701--712. SIAM, 2010.

\bibitem[Baltrunas et~al.(2011)Baltrunas, Kaminskas, Ludwig, Moling, Ricci,
  Aydin, L{\"u}ke, and Schwaiger]{baltrunas2011incarmusic}
Linas Baltrunas, Marius Kaminskas, Bernd Ludwig, Omar Moling, Francesco Ricci,
  Aykan Aydin, Karl-Heinz L{\"u}ke, and Roland Schwaiger.
\newblock Incarmusic: Context-aware music recommendations in a car.
\newblock In \emph{International Conference on Electronic Commerce and Web
  Technologies}, pages 89--100. Springer, 2011.

\bibitem[Bhaskar(2016)]{bhaskar2016probabilistic}
Sonia~A Bhaskar.
\newblock Probabilistic low-rank matrix completion from quantized measurements.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2131--2164, 2016.

\bibitem[Bhaskar and Javanmard(2015)]{bhaskar20151}
Sonia~A Bhaskar and Adel Javanmard.
\newblock 1-bit matrix completion under exact low-rank constraint.
\newblock In \emph{2015 49th Annual Conference on Information Sciences and
  Systems (CISS)}, pages 1--6. IEEE, 2015.

\bibitem[Cai and Zhou(2013)]{cai2013max}
Tony Cai and Wen-Xin Zhou.
\newblock A max-norm constrained minimization approach to 1-bit matrix
  completion.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 3619--3647, 2013.

\bibitem[Chen et~al.(2019)Chen, Raskutti, and Yuan]{chen2019non}
Han Chen, Garvesh Raskutti, and Ming Yuan.
\newblock Non-convex projected gradient descent for generalized low-rank tensor
  regression.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 172--208, 2019.

\bibitem[Davenport et~al.(2014)Davenport, Plan, Van Den~Berg, and
  Wootters]{davenport2014}
Mark~A Davenport, Yaniv Plan, Ewout Van Den~Berg, and Mary Wootters.
\newblock 1-bit matrix completion.
\newblock \emph{Information and Inference: A Journal of the IMA}, 3\penalty0
  (3):\penalty0 189--223, 2014.

\bibitem[Ghadermarzy et~al.(2018)Ghadermarzy, Plan, and
  Yilmaz]{ghadermarzy2018learning}
Navid Ghadermarzy, Yaniv Plan, and Ozgur Yilmaz.
\newblock Learning tensors from partial binary measurements.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (1):\penalty0 29--40, 2018.

\bibitem[Ghadermarzy et~al.(2019)Ghadermarzy, Plan, and
  Yilmaz]{ghadermarzy2019near}
Navid Ghadermarzy, Yaniv Plan, and {\"O}zg{\"u}r Yilmaz.
\newblock Near-optimal sample complexity for convex tensor completion.
\newblock \emph{Information and Inference: A Journal of the IMA}, 8\penalty0
  (3):\penalty0 577--619, 2019.

\bibitem[Hitchcock(1927)]{hitchcock1927expression}
Frank~L Hitchcock.
\newblock The expression of a tensor or a polyadic as a sum of products.
\newblock \emph{Journal of Mathematics and Physics}, 6\penalty0 (1-4):\penalty0
  164--189, 1927.

\bibitem[Hong et~al.(2019)Hong, Kolda, and Duersch]{hong2018generalized}
David Hong, Tamara~G Kolda, and Jed~A Duersch.
\newblock Generalized canonical polyadic tensor decomposition.
\newblock \emph{SIAM Review. In press. arXiv:1808.07452}, 2019.

\bibitem[Hore et~al.(2016)Hore, Vi{\~n}uela, Buil, Knight, McCarthy, Small, and
  Marchini]{hore2016tensor}
Victoria Hore, Ana Vi{\~n}uela, Alfonso Buil, Julian Knight, Mark~I McCarthy,
  Kerrin Small, and Jonathan Marchini.
\newblock Tensor decomposition for multiple-tissue gene expression experiments.
\newblock \emph{Nature genetics}, 48\penalty0 (9):\penalty0 1094, 2016.

\bibitem[Kolda and Bader(2009)]{kolda2009tensor}
Tamara~G Kolda and Brett~W Bader.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM Review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[McCullagh(1980)]{mccullagh1980regression}
Peter McCullagh.
\newblock Regression models for ordinal data.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 42\penalty0 (2):\penalty0 109--127, 1980.

\bibitem[Mu et~al.(2014)Mu, Huang, Wright, and Goldfarb]{mu2014square}
Cun Mu, Bo~Huang, John Wright, and Donald Goldfarb.
\newblock Square deal: Lower bounds and improved relaxations for tensor
  recovery.
\newblock In \emph{International Conference on Machine Learning}, pages 73--81,
  2014.

\bibitem[Negahban et~al.(2011)Negahban, Wainwright,
  et~al.]{negahban2011estimation}
Sahand Negahban, Martin~J Wainwright, et~al.
\newblock Estimation of (near) low-rank matrices with noise and
  high-dimensional scaling.
\newblock \emph{The Annals of Statistics}, 39\penalty0 (2):\penalty0
  1069--1097, 2011.

\bibitem[Nickel et~al.(2011)Nickel, Tresp, and Kriegel]{nickel2011three}
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel.
\newblock A three-way model for collective learning on multi-relational data.
\newblock In \emph{International Conference on Machine Learning}, volume~11,
  pages 809--816, 2011.

\bibitem[Sur and Cand{\`e}s(2019)]{sur2019modern}
Pragya Sur and Emmanuel~J Cand{\`e}s.
\newblock A modern maximum-likelihood theory for high-dimensional logistic
  regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (29):\penalty0 14516--14525, 2019.

\bibitem[Tucker(1966)]{tucker1966some}
Ledyard~R Tucker.
\newblock Some mathematical notes on three-mode factor analysis.
\newblock \emph{Psychometrika}, 31\penalty0 (3):\penalty0 279--311, 1966.

\bibitem[Van~Essen et~al.(2013)Van~Essen, Smith, Barch, Behrens, Yacoub,
  Ugurbil, Consortium, et~al.]{van2013wu}
David~C Van~Essen, Stephen~M Smith, Deanna~M Barch, Timothy~EJ Behrens, Essa
  Yacoub, Kamil Ugurbil, Wu-Minn~HCP Consortium, et~al.
\newblock The {WU-M}inn human connectome project: an overview.
\newblock \emph{Neuroimage}, 80:\penalty0 62--79, 2013.

\bibitem[Wang and Li(2018)]{wang2018learning}
Miaoyan Wang and Lexin Li.
\newblock Learning from binary multiway data: Probabilistic tensor
  decomposition and its statistical optimality.
\newblock \emph{arXiv preprint arXiv:1811.05076}, 2018.

\bibitem[Wang and Zeng(2019)]{zeng2019multiway}
Miaoyan Wang and Yuchen Zeng.
\newblock Multiway clustering via tensor block models.
\newblock \emph{Advances in Neural Information Processing Systems 32 (NeurIPS
  2019). In press. arXiv:1906.03807}, 2019.

\bibitem[Xia and Zhou(2019)]{xia2019sup}
Dong Xia and Fan Zhou.
\newblock The sup-norm perturbation of hosvd and low rank tensor denoising.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (61):\penalty0 1--42, 2019.

\bibitem[Yuan and Zhang(2016)]{yuan2016tensor}
Ming Yuan and Cun-Hui Zhang.
\newblock On tensor completion via nuclear norm minimization.
\newblock \emph{Foundations of Computational Mathematics}, 16\penalty0
  (4):\penalty0 1031--1068, 2016.

\bibitem[Zhang et~al.(2019)]{zhang2019cross}
Anru Zhang et~al.
\newblock Cross: Efficient low-rank tensor completion.
\newblock \emph{The Annals of Statistics}, 47\penalty0 (2):\penalty0 936--964,
  2019.

\bibitem[Zhou et~al.(2013)Zhou, Li, and Zhu]{zhou2013tensor}
Hua Zhou, Lexin Li, and Hongtu Zhu.
\newblock Tensor regression with applications in neuroimaging data analysis.
\newblock \emph{Journal of the American Statistical Association}, 108\penalty0
  (502):\penalty0 540--552, 2013.

\end{thebibliography}
