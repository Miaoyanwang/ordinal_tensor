\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baltrunas2011incarmusic}
\citation{nickel2011three}
\citation{hore2016tensor}
\citation{zhou2013tensor}
\citation{xia2019sup,zeng2019multiway}
\citation{ghadermarzy2018learning,ghadermarzy2019near}
\citation{kolda2009tensor,acar2010scalable}
\citation{hitchcock1927expression}
\citation{tucker1966some}
\citation{kolda2009tensor,ghadermarzy2019near}
\citation{ghadermarzy2018learning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{cai2013max,davenport2014,bhaskar20151}
\citation{bhaskar2016probabilistic}
\citation{wang2018learning,hong2018generalized,ghadermarzy2018learning}
\citation{mccullagh1980regression}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  Comparison with previous work. We summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode. \relax }}{2}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:compare}{{1}{2}{\footnotesize Comparison with previous work. We summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode. \relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Model formulation and motivation}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation model}{2}{subsection.3.1}\protected@file@percent }
\newlabel{eq:model}{{1}{2}{Observation model}{equation.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\citation{mccullagh1980regression}
\citation{zhou2013tensor,bhaskar20151}
\citation{kolda2009tensor}
\newlabel{eq:logodd}{{2}{3}{Observation model}{equation.3.2}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Latent-variable interpretation}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:latent}{{3.2}{3}{Latent-variable interpretation}{subsection.3.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:quantization}{{3}{3}{Latent-variable interpretation}{equation.3.3}{}}
\newlabel{eq:latent}{{4}{3}{Latent-variable interpretation}{equation.3.4}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\newlabel{ass:link}{{1}{3}{}{assumption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Problem 1: Tensor denoising}{3}{subsection.3.3}\protected@file@percent }
\newlabel{sec:denoising}{{3.3}{3}{Problem 1: Tensor denoising}{subsection.3.3}{}}
\newlabel{eq:space}{{3.3}{3}{Problem 1: Tensor denoising}{subsection.3.3}{}}
\newlabel{eq:Tucker}{{5}{3}{Problem 1: Tensor denoising}{equation.3.5}{}}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Problem 2: Tensor completion}{4}{subsection.3.4}\protected@file@percent }
\MT@newlabel{eq:model}
\@writefile{toc}{\contentsline {section}{\numberline {4}Rank-constrained M-estimator}{4}{section.4}\protected@file@percent }
\newlabel{sec:theory}{{4}{4}{Rank-constrained M-estimator}{section.4}{}}
\newlabel{eq:objective}{{6}{4}{Rank-constrained M-estimator}{equation.4.6}{}}
\newlabel{eq:estimator}{{7}{4}{Rank-constrained M-estimator}{equation.4.7}{}}
\newlabel{eq:regular}{{4}{4}{Rank-constrained M-estimator}{equation.4.7}{}}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Estimation error for tensor denoising}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:denosing}{{4.1}{4}{Estimation error for tensor denoising}{subsection.4.1}{}}
\MT@newlabel{eq:estimator}
\newlabel{thm:rate}{{4.1}{4}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:estimator}
\newlabel{eq:rate}{{8}{4}{Statistical convergence}{equation.4.8}{}}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:rate}
\newlabel{eq:ours}{{4.1}{5}{Estimation error for tensor denoising}{equation.4.8}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:estimator}
\newlabel{thm:minimax}{{4.2}{5}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:lower}{{4.2}{5}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:estimator}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sample complexity for tensor completion}{5}{subsection.4.2}\protected@file@percent }
\newlabel{eq:weighted}{{9}{5}{Sample complexity for tensor completion}{equation.4.9}{}}
\MT@newlabel{eq:weighted}
\citation{mu2014square}
\citation{yuan2016tensor}
\citation{zhang2019cross}
\citation{ghadermarzy2018learning}
\newlabel{thm:completion}{{4.3}{6}{}{thm.4.3}{}}
\MT@newlabel{eq:estimator}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Implementation}{6}{section.5}\protected@file@percent }
\newlabel{sec:algorithm}{{5}{6}{Numerical Implementation}{section.5}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:estimator}
\newlabel{fig:stability}{{5}{6}{Numerical Implementation}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  Trajectory of objective function with the input tensor generated from probit model\nobreakspace  {}\MT_extended_eqref:n  {eq:model} with $d_1=d_2=d_3=d$ and $r_1=r_2=r_3=r$. The dashed line is the objective value at the true parameter $\mathcal  {L}_{\mathcal  {Y},\Omega }(\Theta ^{\text  {true}})$.\relax }}{6}{figure.1}\protected@file@percent }
\MT@newlabel{eq:model}
\newlabel{eq:BIC}{{5}{6}{Numerical Implementation}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{6}{section.6}\protected@file@percent }
\newlabel{sec:experiment}{{6}{6}{Experiments}{section.6}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:Tucker}
\citation{davenport2014,sur2019modern}
\citation{acar2010scalable}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Finite-sample performance}{7}{subsection.6.1}\protected@file@percent }
\newlabel{sec:simulation}{{6.1}{7}{Finite-sample performance}{subsection.6.1}{}}
\MT@newlabel{eq:rate}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:finite}{{2}{7}{\footnotesize Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with alternative methods}{7}{subsection.6.2}\protected@file@percent }
\MT@newlabel{eq:model}
\citation{van2013wu}
\citation{baltrunas2011incarmusic}
\citation{chen2019non}
\bibstyle{plain}
\bibdata{tensor_wang.bib}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  Performance comparison in MCR (a, b) and MAD (c, d). (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. (a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. \relax }}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:compare}{{3}{8}{\footnotesize Performance comparison in MCR (a, b) and MAD (c, d). (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. (a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. \relax }{figure.caption.4}{}}
\MT@newlabel{eq:logodd}
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Applications}{8}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  Comparison of prediction error in the HPC and InCarMusic. Standard errors are reported in parentheses.\relax }}{8}{table.caption.5}\protected@file@percent }
\newlabel{table:CV}{{2}{8}{\footnotesize Comparison of prediction error in the HPC and InCarMusic. Standard errors are reported in parentheses.\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions}{8}{section.8}\protected@file@percent }
\bibcite{acar2010scalable}{{1}{2010}{{Acar et~al.}}{{Acar, Dunlavy, Kolda, and M{\o }rup}}}
\bibcite{baltrunas2011incarmusic}{{2}{2011}{{Baltrunas et~al.}}{{Baltrunas, Kaminskas, Ludwig, Moling, Ricci, Aydin, L{\"u}ke, and Schwaiger}}}
\bibcite{bhaskar2016probabilistic}{{3}{2016}{{Bhaskar}}{{}}}
\bibcite{bhaskar20151}{{4}{2015}{{Bhaskar and Javanmard}}{{}}}
\bibcite{cai2013max}{{5}{2013}{{Cai and Zhou}}{{}}}
\bibcite{chen2019non}{{6}{2019}{{Chen et~al.}}{{Chen, Raskutti, and Yuan}}}
\bibcite{davenport2014}{{7}{2014}{{Davenport et~al.}}{{Davenport, Plan, Van Den~Berg, and Wootters}}}
\bibcite{ghadermarzy2018learning}{{8}{2018}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{ghadermarzy2019near}{{9}{2019}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{hitchcock1927expression}{{10}{1927}{{Hitchcock}}{{}}}
\bibcite{hong2018generalized}{{11}{2019}{{Hong et~al.}}{{Hong, Kolda, and Duersch}}}
\bibcite{hore2016tensor}{{12}{2016}{{Hore et~al.}}{{Hore, Vi{\~n}uela, Buil, Knight, McCarthy, Small, and Marchini}}}
\bibcite{kolda2009tensor}{{13}{2009}{{Kolda and Bader}}{{}}}
\bibcite{mccullagh1980regression}{{14}{1980}{{McCullagh}}{{}}}
\bibcite{mu2014square}{{15}{2014}{{Mu et~al.}}{{Mu, Huang, Wright, and Goldfarb}}}
\bibcite{negahban2011estimation}{{16}{2011}{{Negahban et~al.}}{{Negahban, Wainwright, et~al.}}}
\bibcite{nickel2011three}{{17}{2011}{{Nickel et~al.}}{{Nickel, Tresp, and Kriegel}}}
\bibcite{sur2019modern}{{18}{2019}{{Sur and Cand{\`e}s}}{{}}}
\bibcite{tucker1966some}{{19}{1966}{{Tucker}}{{}}}
\bibcite{van2013wu}{{20}{2013}{{Van~Essen et~al.}}{{Van~Essen, Smith, Barch, Behrens, Yacoub, Ugurbil, Consortium, et~al.}}}
\bibcite{wang2018learning}{{21}{2018}{{Wang and Li}}{{}}}
\bibcite{zeng2019multiway}{{22}{2019}{{Wang and Zeng}}{{}}}
\bibcite{xia2019sup}{{23}{2019}{{Xia and Zhou}}{{}}}
\bibcite{yuan2016tensor}{{24}{2016}{{Yuan and Zhang}}{{}}}
\bibcite{zhang2019cross}{{25}{2019}{{Zhang et~al.}}{{}}}
\bibcite{zhou2013tensor}{{26}{2013}{{Zhou et~al.}}{{Zhou, Li, and Zhu}}}
