Reviewer #5
Q1. Distinction between classical tensors and ordinal tensors 
( because we are using Tucker decomposition?, He or she wants clear distinction and challenges with regards to ordinal tensor, is it special case of classical analysis)

A1. Our ordinal tensor analysis is conceptually different from continuous tensor analysis method. We make use of quantization to guarantee categorical responses. This quantization model ensure that the estimation is invariant under a reversal of categories and the parameter interpretations are consistent, which is not the case in classical tensor analysis. Our method is not a special case of classical analysis, but it is one step more from classical analysis. In addition, this latent model helps us to interpret the model with signal and noise concept. With this concept, the model has the phenomenon that estimation becomes harder in high signal level in Figure 2-b which is distinctive from classical analysis.
(Also, it is not exponential family)

Q2. The tools adopted is from classical tensor analysis (rank constrained M estimation) -This also can be related to reviewer #3’s question,

A2. I agree that rank constrained M estimation is common in previous works. In our paper, however, we established the theoretical results guaranteeing optimal convergence and complexity of M estimation. Those theories make our paper distinctive from others and justify using constrained M estimation.


Reviewer #4
Q1. The initialization of “b” is quite important to obtain good local optima. Do you have any good strategy to initialize b?(Initialization problem, may be the reviewer concerns identifiability issue too.)

A1.First, we perform multiple initialization methods to increase the probability of getting good initialization. Actual value of threshold parameter b cannot be decided by itself but with consideration of signal tensor and response variables, which means if signal tensor shifted left then b will follow (consider P(y\geq l) = f(b_l-theta)). Identifiability issue can arise from this point. So we set signal tensor sum to zero to solve the identifiability issue. Since we have mean 0 signal tensor, it is natiural for us to initialize the threshold parameter b as an increasing sequence such that the cumulative density of the logistic distribution between consecutative threshold is the same (or an increasing seqeunece such that the cumulative density of the logistic distribution between consecutative threshold reflects the proportion of ordinal values in the data). This initialization strategy for b combining with multiple initialization makes algorithm stable and start in good position to obtain good local optima. 

Q2. Computational cost for the optimization.
MovieLens: dataset represents rating tuples of the form: <user, item, rating, timestamp>. There are diverse datasets such as 
100k:100,000 ratings from 1000 users on 1700 movies
1M:1 million ratings from 6000 users on 4000 movies
10M:10 million ratings and 100,000 tag applications applied to 10,000 movies by 72,000 users
20M:20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users
https://papers.nips.cc/paper/8860-singleshot-a-scalable-tucker-tensor-decomposition.pdf (Traore, A., Bérar, M., & Rakotomamonjy, A. (2019). Singleshot : a scalable Tucker tensor decomposition. NeurIPS.)
https://arxiv.org/pdf/1710.02261v1.pdf(Oh, S., Park, N., Sael, L., & Kang, U. (2018). Scalable Tucker Factorization for Sparse Tensors - Algorithms and Discoveries. 2018 IEEE 34th International Conference on Data Engineering (ICDE), 1120-1131.)
: uses data where 6040 users, 100209 ratings from 1 to 6 on 3076 movies , between April,2000 and February,2003
(It assumes sparse tensor which means rank is really low)

A2. For bigger dataset, like Movielens dataset, we can use small rank to reduce computations like other papers have done for the application on Movielens such as
(Traore, A., Bérar, M., & Rakotomamonjy, A. (2019). Singleshot : a scalable Tucker tensor decomposition. NeurIPS,
Oh, S., Park, N., Sael, L., & Kang, U. (2018). Scalable Tucker Factorization for Sparse Tensors - Algorithms and Discoveries. 2018 IEEE 34th International Conference on Data Engineering (ICDE), 1120-1131.) Here they used Tucker rank (5,5,5) and (10,10,10). This small rank makes implementation much faster (our brain dataset rank was (23,23,8) which is not quite small)
There are several rooms for us to improve our current algorithm for big data applications. Movielens datasets have more than 100000 ratings which makes computation of gradient challenging. However our loss function has good structure to apply stochastic gradient descent method. SGD is one way to improve our computation time. In addition, we can select a row-wise update method for factor matrices. In our alternating gradient based algorithm, we can show that all rows of factor matrices are independent of each other in the sense of minimizing our loss function. This property enables us to use parallel computing which significantly reduces memory requirement for updating factor matrices. I think those approaches enable us to apply big dataset like MovieLens.



Reviewer #3
Q1. Can not find tractable algorithms for MLE ( He or she concerns about the gap between theoretical result and real application)

A1.
It is fair argument. However, I want to emphasize that our theorems are not only applicable to the global optimal MLE but any local optimums suffice as long as it has smaller loss function than at true parameter. This implies that the local optimums to the MLE have the optimal rate in the condition. Therefore, I would say that the optimality of the MLE is still useful. In the same line,  we argue that the finding a local minimum is suffice not only with experiments but also with theoretical analysis in our proof. 
The experiments are coincide with the theorems in the sense that our approach with possible local minimas outperforms other approaches (optimal vs suboptimal). 

As you mentioned, the sufficiency of the finding local optima of nonconvex problems is very active area. Most of works are focusing on showing that local optima is global optima. Those approaches are different from our approach but wil provide another good point of our estimation method.

In the matrix case, many papers have shown that one would expect to achieve global convergence for the rank constrained optimization problem provided algorithms converge to a local minimum.
(R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in Neural Information Processing Systems, 2008.,
T. Cai and W.-X. Zhou. A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion. Journal of Machine Learning Research, 14:3619–3647, 2013.,
Ge, R., Jin, C., & Zheng, Y. (2017, August). No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1233-1242). JMLR. org.)

In the tensor case, many studies try to show there is no spurious local minima for the rank constrained optimization problem.
(Haeffele, B. D., & Vidal, R. (2015). Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540.,
Ge, R., & Ma, T. (2017). On the optimization landscape of tensor decompositions. In Advances in Neural Information Processing Systems (pp. 3653-3663).,
Sanjabi, M., Baharlouei, S., Razaviyayn, M., & Lee, J. D. (2019). When Does Non-Orthogonal Tensor Decomposition Have No Spurious Local Minima?. arXiv preprint arXiv:1911.09815.
). 
Even though the optimization problems in those papers are the special case of our problem (for example assuming that factor matrices are identical or adding extra regularization term), they are giving us positive conjecture that our algorithm may succeed finding global minima.



