\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baltrunas2011incarmusic}
\citation{nickel2011three}
\citation{hore2016tensor}
\citation{zhou2013tensor}
\citation{xia2019sup,zeng2019multiway}
\citation{ghadermarzy2018learning,ghadermarzy2019near}
\citation{kolda2009tensor,acar2010scalable}
\citation{hitchcock1927expression}
\citation{tucker1966some}
\citation{kolda2009tensor,ghadermarzy2019near}
\citation{ghadermarzy2018learning}
\citation{cai2013max,davenport2014,bhaskar20151}
\citation{bhaskar2016probabilistic}
\citation{wang2018learning,hong2018generalized,ghadermarzy2018learning}
\citation{mccullagh1980regression}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\newlabel{tab:compare}{{1}{2}{Comparison with previous work. For ease of presentation, we summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode}{table.1}{}}
\newlabel{eq:model}{{1}{2}{}{equation.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{eq:logodd}{{2}{2}{}{equation.3.2}{}}
\MT@newlabel{eq:model}
\citation{mccullagh1980regression}
\citation{zhou2013tensor,bhaskar20151}
\citation{kolda2009tensor}
\citation{hitchcock1927expression}
\citation{oseledets2011tensor}
\citation{de2008tensor}
\MT@newlabel{eq:logodd}
\newlabel{sec:latent}{{3.2}{3}{}{subsection.3.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:quantization}{{3}{3}{}{equation.3.3}{}}
\newlabel{eq:latent}{{4}{3}{}{equation.3.4}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\newlabel{ass:link}{{1}{3}{}{assumption.1}{}}
\newlabel{sec:denoising}{{3.3}{3}{}{subsection.3.3}{}}
\newlabel{eq:space}{{5}{3}{}{equation.3.5}{}}
\newlabel{eq:Tucker}{{6}{3}{}{equation.3.6}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:model}
\citation{negahban2011estimation,cai2013max,bhaskar20151}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\MT@newlabel{eq:space}
\newlabel{sec:theory}{{4}{4}{}{section.4}{}}
\newlabel{eq:objective}{{7}{4}{}{equation.4.7}{}}
\newlabel{eq:estimator}{{8}{4}{}{equation.4.8}{}}
\newlabel{eq:regular}{{4}{4}{}{equation.4.8}{}}
\MT@newlabel{eq:latent}
\newlabel{sec:denosing}{{4.1}{4}{}{subsection.4.1}{}}
\MT@newlabel{eq:estimator}
\newlabel{thm:rate}{{4.1}{4}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:estimator}
\newlabel{eq:rate}{{9}{4}{Statistical convergence}{equation.4.9}{}}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:estimator}
\newlabel{cor:prediction}{{1}{4}{Prediction error}{cor.1}{}}
\newlabel{eq:KLrate}{{10}{4}{Prediction error}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\newlabel{eq:ours}{{4.1}{4}{}{equation.4.10}{}}
\citation{bhaskar2016probabilistic}
\citation{mu2014square}
\citation{yuan2016tensor}
\citation{zhang2019cross}
\citation{ghadermarzy2018learning}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:KLrate}
\MT@newlabel{eq:estimator}
\newlabel{thm:minimax}{{4.2}{5}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:lower}{{4.2}{5}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:estimator}
\newlabel{eq:weighted}{{11}{5}{}{equation.4.11}{}}
\MT@newlabel{eq:weighted}
\newlabel{thm:completion}{{4.3}{5}{}{thm.4.3}{}}
\MT@newlabel{eq:estimator}
\newlabel{sec:algorithm}{{5}{5}{}{section.5}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:Tucker}
\MT@newlabel{eq:objective}
\citation{davenport2014,sur2019modern}
\newlabel{alg}{{1}{6}{}{algorithm.1}{}}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:model}
\newlabel{fig:stability}{{5}{6}{}{ALC@unique.13}{}}
\newlabel{eq:BIC}{{5}{6}{}{figure.1}{}}
\newlabel{sec:experiment}{{6}{6}{}{section.6}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:Tucker}
\newlabel{sec:simulation}{{6.1}{6}{}{subsection.6.1}{}}
\MT@newlabel{eq:rate}
\citation{acar2010scalable}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\newlabel{fig:finite}{{2}{7}{Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\FnormSize {}{\hat \Theta -\trueT }/\FnormSize {}{\trueT }$ for better visualization}{figure.2}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\MT@newlabel{eq:latent}
\citation{geddes2016human}
\citation{baltrunas2011incarmusic}
\citation{baltrunas2011incarmusic}
\citation{chen2019non}
\newlabel{fig:compare}{{3}{8}{Performance comparison in MCR (a, b) and MAD (c, d). (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. (a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$}{figure.3}{}}
\newlabel{table:CV}{{2}{8}{Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses}{table.2}{}}
\bibdata{ordinalTv3.bbl}
\bibcite{acar2010scalable}{{1}{2010}{{Acar et~al.}}{{Acar, Dunlavy, Kolda, and M{\o }rup}}}
\bibcite{baltrunas2011incarmusic}{{2}{2011}{{Baltrunas et~al.}}{{Baltrunas, Kaminskas, Ludwig, Moling, Ricci, Aydin, L{\"u}ke, and Schwaiger}}}
\bibcite{bhaskar2016probabilistic}{{3}{2016}{{Bhaskar}}{{}}}
\bibcite{bhaskar20151}{{4}{2015}{{Bhaskar \& Javanmard}}{{Bhaskar and Javanmard}}}
\bibcite{cai2013max}{{5}{2013}{{Cai \& Zhou}}{{Cai and Zhou}}}
\bibcite{chen2019non}{{6}{2019}{{Chen et~al.}}{{Chen, Raskutti, and Yuan}}}
\bibcite{davenport2014}{{7}{2014}{{Davenport et~al.}}{{Davenport, Plan, Van Den~Berg, and Wootters}}}
\bibcite{de2008tensor}{{8}{2008}{{De~Silva \& Lim}}{{De~Silva and Lim}}}
\bibcite{geddes2016human}{{9}{2016}{{Geddes}}{{}}}
\bibcite{ghadermarzy2018learning}{{10}{2018}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{ghadermarzy2019near}{{11}{2019}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{hitchcock1927expression}{{12}{1927}{{Hitchcock}}{{}}}
\bibcite{hong2018generalized}{{13}{2019}{{Hong et~al.}}{{Hong, Kolda, and Duersch}}}
\bibcite{hore2016tensor}{{14}{2016}{{Hore et~al.}}{{Hore, Vi{\~n}uela, Buil, Knight, McCarthy, Small, and Marchini}}}
\bibcite{kolda2009tensor}{{15}{2009}{{Kolda \& Bader}}{{Kolda and Bader}}}
\bibcite{mccullagh1980regression}{{16}{1980}{{McCullagh}}{{}}}
\bibcite{mu2014square}{{17}{2014}{{Mu et~al.}}{{Mu, Huang, Wright, and Goldfarb}}}
\bibcite{negahban2011estimation}{{18}{2011}{{Negahban et~al.}}{{Negahban, Wainwright, et~al.}}}
\bibcite{nickel2011three}{{19}{2011}{{Nickel et~al.}}{{Nickel, Tresp, and Kriegel}}}
\bibcite{oseledets2011tensor}{{20}{2011}{{Oseledets}}{{}}}
\bibcite{sur2019modern}{{21}{2019}{{Sur \& Cand{\`e}s}}{{Sur and Cand{\`e}s}}}
\bibcite{tucker1966some}{{22}{1966}{{Tucker}}{{}}}
\bibcite{wang2018learning}{{23}{2018}{{Wang \& Li}}{{Wang and Li}}}
\bibcite{zeng2019multiway}{{24}{2019}{{Wang \& Zeng}}{{Wang and Zeng}}}
\bibcite{xia2019sup}{{25}{2019}{{Xia \& Zhou}}{{Xia and Zhou}}}
\bibcite{yuan2016tensor}{{26}{2016}{{Yuan \& Zhang}}{{Yuan and Zhang}}}
\bibcite{zhang2019cross}{{27}{2019}{{Zhang et~al.}}{{}}}
\bibcite{zhou2013tensor}{{28}{2013}{{Zhou et~al.}}{{Zhou, Li, and Zhu}}}
\bibstyle{icml2020}
