{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red74\green74\blue74;\red0\green0\blue233;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c36078\c36078\c36078;\cssrgb\c0\c0\c93333;
}
\margl1440\margr1440\vieww22180\viewh13580\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\b\fs24 \cf2 \expnd0\expndtw0\kerning0
Reviewer #5
\f1\b0 \
Q1. Distinction between classical tensors and ordinal tensors ( because we are using Tucker decomposition?, He or she wants clear distinction and challenges with regards to ordinal tensor, is it special case of classical analysis?) \
\
A1. Our ordinal tensor analysis is conceptually different from continuous tensor analysis method. We make use of quantization to guarantee categorical responses. This quantization model ensure that the estimation is invariant under a reversal of categories and the parameter interpretations are consistent, which is not the case in classical tensor analysis. Our method is not a special case of classical analysis, but it is one step more from classical analysis. In addition, this latent model helps us to interpret the model with signal and noise concept. With this concept, the model has the phenomenon that estimation becomes harder in high signal level in Figure 2-b which is distinctive from classical analysis. \
(Also, it is not exponential family) \
\
Q2. The tools adopted is from classical tensor analysis (rank constrained M estimation) -
\f0\b This also can be related to reviewer #3\'92s question,
\f1\b0  \
\
A2. I agree that rank constrained M estimation is common in previous works. In our paper, however, we established the theoretical results guaranteeing optimal convergence and complexity of M estimation. Those theories make our paper distinctive from others and justify using constrained M estimation. \
\
\

\f0\b Reviewer #4
\f1\b0 \
Q1. The initialization of \'93b\'94 is quite important to obtain good local optima. Do you have any good strategy to initialize b?(The reviewer is curious about general Initialization method and may be concerns identifiability issue  related to the intercept \'93b\'94.) \
\
A1.First, we perform multiple initialization methods to increase the probability of getting good initialization. Actual value of intercept b cannot be decided by itself but with consideration of signal tensor and response variables, which means if signal tensor shifted left then b will follow. Identifiability issue can arise from this point. So we set signal tensor sum to zero for each update and update intercept b to solve identifiability issue. In this way, we initialize intercept b to be fitted on the initialization of signal tensor which sum to zero. \'a0This initialization strategy for b combining with multiple initialization makes algorithm stable and start in good position to obtain good local optima.\'a0 \
\
Q2. Computational cost for the optimization. \
Here is MovieLens data structure.\
MovieLens: dataset represents rating tuples of the form: <user, item, rating, timestamp>. There are diverse datasets such as\'a0 \
100k:\cf3 100,000 ratings from 1000 users on 1700 movies\cf2  \
1M:\cf3 1 million ratings from 6000 users on 4000 movies\cf2  \
10M:\cf3 10 million ratings and 100,000 tag applications applied to 10,000 movies by 72,000 users\cf2  \
20M:\cf3 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users\cf2  \
One article that uses MovieLens data in tensor application,\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://ieeexplore-ieee-org.ezproxy.library.wisc.edu/stamp/stamp.jsp?tp=&arnumber=6005968"}}{\fldrslt \cf4 \ul \ulc4 https://ieeexplore-ieee-org.ezproxy.library.wisc.edu/stamp/stamp.jsp?tp=&arnumber=6005968}}\
: uses data where 6040 users, 100209 ratings from 1 to 6 on 3076 movies , between April,2000 and February,2003 \
(It assumes sparse tensor which means rank is really low) \
\
A2. For bigger dataset, like MovieLens dataset, we can use sparse tensor structure by imposing small rank to reduce computations like other papers have done for the application on MovieLens ({\field{\*\fldinst{HYPERLINK "https://ieeexplore-ieee-org.ezproxy.library.wisc.edu/stamp/stamp.jsp?tp=&arnumber=6005968"}}{\fldrslt \cf3 \ul \ulc3 https://ieeexplore-ieee-org.ezproxy.library.wisc.edu/stamp/stamp.jsp?tp=&arnumber=6005968}}). This small rank makes implementation time much faster (our brain dataset rank was (23,23,8) which is not quite small) \
There are several rooms for us to improve our current algorithm for big data applications. MovieLens datasets have more than 100000 ratings which makes computation of gradient challenging. However our loss function has good structure to apply stochastic gradient descent method. With the help fo this, we can overcome computation difficulty. In addition, we can make use of parallel computing in updating each factor matrix. This also helps saving implementation time a lot. \
\
\pard\pardeftab720\partightenfactor0

\f0\b \cf2 Reviewer #3
\f1\b0 \
Q1. Can not find tractable algorithms for MLE (The reviewer concerns about the gap between theoretical result and real application) \
\
A1. \
It is fair argument. However, no other approaches succeed to have tractable algorithm \'a0to estimate global optimizer. The other methods include the nuclear norm approaches sacrificing rank constraints but still having computation hard algorithm. Also, our paper is meaningful in the sense that we show that this straightforward method works better than other approaches even with relaxed local minimum condition. Our paper shows theoretical reason for this outperformance: MLE has optimal convergence rate and optimal complexity which makes even local minimum estimation outperforms the other methods with suboptimal rate. This paper provides one step closer to ultimate goal of tensor completion method for ordinal dataset. (Another fact is multiple initialization can handle local minimization problem too. In addition, It is known that\'a0
\fs26\fsmilli13333 with multiple random restarts and deflation is capable of finding all components
\fs24  
\fs26\fsmilli13333 u
\fs18\fsmilli9333 i
\fs24  
\fs26\fsmilli13333 with high probability?)
\fs24 \
\pard\pardeftab720\partightenfactor0

\fs26\fsmilli13333 \cf2 \
(I am taking a look at articles that study about spurious local minima but can not find good articles to apply in our method yet. I will do work more on that to build answer for this part)
\fs24 \
\
\
}