We thank the reviewers for helpful comments and feedback. Our responses are detailed below.

To Reviewer 3:
* Regarding theoretical contributions and MLE approach

We highlight two main contributions that set our work apart from current literature. 
1) [New challenges with non-classical observations] The problem of ordinal/quantized tensor is fundamentally more challenging than previously well-studied tensors for two reasons: (a) the entries do not belong to exponential family distribution, and (b) the observation contains much less information, because neither the underlying signal nor the quantization operator is known. The limited information makes the statistical recovery notably hard, not to mention the associated computation. 

We are among the first to establish the recovery theory for both the signal tensors and the quantization operators from a limited number of highly discrete entries. Our model achieves invariance under a reversal of categories and consistency under merging/splitting of contiguous categories. A distinctive non-monotonic phase-transition pattern is demonstrated (Figure 2b). None of these features are ever considered in classical tensor analysis. Furthermore, we prove that, surprisingly, recovery from quantized tensors achieves equally good information-theoretical convergence as the continuous tensors. These results fill the gap between classical and non-classical (ordinal) observations, thereby greatly enriching the tensor-model literature. 

2) [New challenges with regularized MLE] The properties of regularized MLE are challenging in our tensor model. Current approaches for solving similar problems fall into two categories: convex (e.g., nuclear-norm regularized) MLE and non-convex MLE (our approach). Most work focuses on the former. However, in the tensor case, convex MLE suffers from both computational in-tractability and statistical sub-optimality. This is because computing nuclear norm is NP-hard for tensors. 

We advocate the non-convex approach and provide strong evidence for its success in our setting. We improve the statistical error bound from d^{-(K-1)/2} to d^{-(K-1)}, which is substantial as the tensor order K increases. This solves the earlier open question whether the square root can be removed in an unlimited computational setting [Ghadermarzy et al 2019]. As a by-effect, we demonstrate the statistical optimality of the non-convex approach over the (computationally intractably) convex MLE approach. While the computational limit remains open (as in many modern ML problems), we believe our improved statistical bound provides a useful benchmark for future algorithm development. 

* Regarding the optimization

We agree with the reviewer that MLE is computationally hard. While our main contributions lie in the aforementioned two aspects, we do provide useful results on optimization. We show that the alternating algorithm convergences to local optimums, and we prove (in Thm 4.1 and Supplements) that the local optimums with reasonably high objective suffice for accurate recovery. In practice, we find that the estimation is robust under randomized initialization, which resembles the `benign non-convexâ€™ phenomenon in literature. In special cases such as rank-1 or orthogonally decomposable tensors, we are able to leverage recent advances on optimization to obtain the asymptotical number of initializations for global guarantee, however, at the cost of more stringent assumptions on the underlying signal tensor. For this reason, we choose not to pursue this direction but leave framework general for the current work. 

Tensor learning is a clear challenge for further research. We believe our work provides a (somewhat surprising) new idea for quantized tensor problem, and we obtain results that were previously difficult to establish. We hope the work open up new inquiry that allows more machine learning researchers to contribute to this field.

To Reviewer 4:
* Initialization of b
We initialize b such that each of the estimated categories has roughly equal size. Furthermore, multiple initializations are utilized and we choose the one that maximizes the objective. We find that such initialization strategy gives stable results in the simulations and the two applications considered. 

* Computational costs
We choose these two datasets for fair comparison because they were used in the previous work. We agree that application to large datasets such as MovieLens would be interesting. In fact, our alternating approach lends itself well to this context. The sub-step for solving tensor factors could be paralleled and be boosted by fast algorithms such as stochastic gradient descent. We would be glad to include this application, space permitting.

To Reviewer 5:
* Difference to classical tensors
See our response to reviewer 3 regarding `New challenges with non-classical observations'

* rank-constrained MLE
See our response to reviewer 3 regarding `New challenges with regularized MLE'
