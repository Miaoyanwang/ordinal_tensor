\documentclass[11pt]{article}

\usepackage{fancybox}
\usepackage{color}
\usepackage{url}
\usepackage[margin=1in]{geometry}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{natbib}
\usepackage{dsfont,multirow,hyperref,setspace,enumerate}
\hypersetup{colorlinks,linkcolor={blue},citecolor={blue},urlcolor={red}}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}

\setlength\parindent{0pt}
\usepackage[parfill]{parskip}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}


\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{dsfont}
\usepackage{wrapfig}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\newcommand*{\KeepStyleUnderBrace}[1]{%f
 \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\begingroup
    \makeatletter
    \@for\theoremstyle:=definition,remark,plain\do{%
        \expandafter\g@addto@macro\csname th@\theoremstyle\endcsname{%
            \addtolength\thm@preskip\parskip
            }%
        }
\endgroup



\input macros.tex
\allowdisplaybreaks



\begin{document}
\begin{center}
{\large Joint estimation of $\hat \Theta$ and $\hat \mb$ in the tensor ordinal model}\\
\vspace{.1cm}
Miaoyan Wang, Feb 14, 2020. 
\end{center}
\begin{rmk}[Notation]
For notational convenience, we consider the special case $d_1=\cdots=d_K=d$. We focus on the asymptotic region $d\to \infty$ and treat all other quantities as constants, i.e., $\mr, L, U_{\alpha,\beta,\Delta}, L_{\alpha,\beta,\Delta},...=\tO(1)$. In particular, 
\begin{enumerate}
\item $\prod_k d_k \asymp d^K$, $\sum_k d_k\asymp d$. 
\item Denote $n_{\max}=\max_\ell n_{\ell}$ and $n_{\min}=\min_\ell(n_\ell+n_{\ell+1})$. Then, $n_{\max}\asymp d^K$ (important!) but $\tO(1)\leq n_{\min}\leq \tO(d^K)$. 
\end{enumerate}
Note: we use $C>$ to denote a positive constant whose value may change from line to line. 
\end{rmk}

\section{Results}
\subsection{Current bound}
Total MSE:
\begin{align}\label{eq:bound1}
\text{MSE}((\hat \Theta,\hat \mb), (\trueT,\trueb))&\stackrel{\text{def}}{=}{\prod_k d_k \mathrm{MSE}(\hat \Theta, \trueT)+(L-1)\mathrm{MSE}(\hat \Theta, \trueT) \over \prod_k d_k+L-1} \notag \\
&\leq {c_1\sum_k d_k + c_2  {n^2_{\max} \over n^2_{\min}} \over \prod_k d_k+L-1} \notag \\
&=\tO\left({\sum_k d_k\over \prod_k d_k}\right)+\tO\left({n^2_{\max} \over n^2_{\min}} {1\over \prod_k d_k}\right)
\end{align}


\begin{rmk}
Unfortunately, this total MSE bound does not converge to zero. In the worse case, the bound can be $\asymp d^K$;  for example, the bound diverges when $n_{\min}\asymp 1$ (and $n_{\max}\asymp d^K$). 
\end{rmk}
\begin{rmk}
The total MSE bound converges to zero only when 
\begin{equation}\label{eq:1}
{n_{\max}\over n_{\min}}\ll d^{K/2},\quad \text{or equivalently,}\quad n_{\min} \gg d^{K/2}.
\end{equation}
In other words, the current bound~\eqref{eq:bound1} tolerates only certain imbalanced classes for which $\tO(d^{K/2})\leq n_{\min}\leq \tO(d^K)$. 
\end{rmk}


\subsection{Sharper bound}
We will prove a sharper bound on the linear term
\begin{align}\label{eq:linearb}
(\trueb-\hat\mb)^T\fploglb(\trueb)&\leq \FnormSize{}{\trueb-\hat\mb}\FnormSize{}{\fploglb(\trueb)}\\
&\leq C\FnormSize{}{\trueb-\hat\mb} \sqrt{d^{K+2}},
\end{align}
where the last inequality is followed from a sharper bound on the gradient:
\begin{equation}\label{eq:bound}
\left|{\partial \tL_\tY\over \partial b_\ell}\Big|_{(\hat \Theta, \trueb)}\right|   \leq C \sqrt{d^{K+2}}, \quad \textrm{ for all } \; \ell\in [L-1].
\end{equation}
Suppose~\eqref{eq:bound} holds. Following the same line in the current proof (i.e.,\ Taylor expansion, quadratic bound, etc.), we have
\[
\FnormSize{}{\trueb-\hat \mb}\leq C{d^{(K+2)/2}  \over n_{\min}}.
\]
{\color{red} (Final results.) Therefore, the total MSE:
\begin{equation}\label{eq:boundnew}
\text{Total MSE}\leq \tO\left({1\over d^{K-1}}\right)+\tO\left(d^{K+2}\over n^2_{\min} d^K\right)=\tO\left({d^2\over \min\{d^{K+1}, n^2_{\min}\}}\right)
\end{equation}
which convergences to zero whenever
\begin{equation}\label{eq:requirement}
n_{\min} \gg d.
\end{equation}
}
In other words, the new bound~\eqref{eq:boundnew} tolerates highly imbalanced classes for which $\tO(\sqrt{d})\leq n_{\min}\leq \tO(d^K)$.

\begin{rmk}
The consistency condition~\eqref{eq:requirement} is more relaxed than~\eqref{eq:1} for all $K\geq 3$. Both bounds agree in the matrix case ($K=2$). 
\end{rmk}

\begin{rmk}
When $n_{\min}\gg d^{(K+1)/2}$, the error in $\hat \Theta$ dominates the total MSE. Then $n_{\min}\ll d^{(K+1)/2}$, the error in $\hat \mb$ dominates the total MSE
\end{rmk}

\section{Proofs}
Now we prove the inequality~\eqref{eq:bound}. 
\begin{lem}[Sharper Bound on Gradients]\label{lem:gradient}
Consider the same set-up as in Theorem A.1. Then, with very high probability,
\[
\left|{\partial \tL_\tY\over \partial b_\ell}\Big|_{(\hat \Theta, \trueb)}\right| \leq C_1d^{K/2}+C_2d^{K/2}\FnormSize{}{\trueT-\hat \Theta},\quad \textrm{ for all } \; \ell\in [L-1].
\]
where $C_1,C_2>0$ are two constants. In particular, there exists a constant $d_0\in\mathbb{N}_{+}$, such that for all $d\geq d_0$,
\begin{equation}
\left|{\partial \tL_\tY\over \partial b_\ell}\Big|_{(\hat \Theta, \trueb)}\right|   \leq Cd^{(K+2)/2},\quad \textrm{ for all } \; \ell\in [L-1].
\end{equation}
\end{lem}

\begin{cor}[MSE for $\hat \mb$] Under the same set-up as in Theorem A.1, we have
\begin{equation}\label{eq:boundb}
\FnormSize{}{\hat \mb-\trueb}\leq { C_1d^{K/2}+C_2d^{K/2}\FnormSize{}{\hat \Theta-\trueT} \over n_{\min}}\leq {Cd^{(K+2)/2}\over n_{\min}}.
\end{equation}
\end{cor}
\begin{rmk} The bound~\eqref{eq:bound} is sharper than the trivial bound $|b^{\text{true}}_\ell-\hat b_\ell|\leq 2\beta$. In particular, $\FnormSize{}{\hat \mb-\trueb}\to 0$ as $n_{\min}\asymp (d^{(K+2)/2})\to \infty$. 
\end{rmk}
\begin{proof}[Proof of Lemma~\ref{lem:gradient}]
We only prove the case for $\ell=1$. Other cases can be proved similarly. 

Note that 
\begin{align}\label{eq:diff}
{\partial \tL_\tY\over \partial b_1}\Big|_{(\hat \Theta, \trueb)}
 &=\KeepStyleUnderBrace{ {\partial \tL_\tY\over \partial b_1}\Big|_{(\hat \Theta, \trueb)}-\mathbb{E}_{\tY}\left[{\partial \tL_\tY\over \partial b_1}\Big|_{(\hat \Theta, \trueb)}\right]}_{\text{:=A}}+\KeepStyleUnderBrace{\mathbb{E}_{\tY}\left[{\partial \tL_\tY\over \partial b_1}\Big|_{(\hat \Theta, \trueb)}\right]-\mathbb{E}_{\tY}\left[{\partial \tL_\tY\over \partial b_1}\Big|_{(\trueT, \trueb)}\right]}_{:=B},
\end{align}
where we have used the fact that the score function has mean zero, $\mathbb{E}_{\tY}\left[{\partial \tL_\tY\over \partial b_1}\Big|_{(\trueT, \trueb)}\right]=0$. Here all expectations are taken with respect to $\tY\sim \mathbb{P}(\trueT, \trueb)$. 

We now bound the two deviation terms in~\eqref{eq:diff} separately.  The term $A$ in~\eqref{eq:diff} is the stochastic deviation of log-likelihood to its expectation:
\begin{align}
A=\sum_{\omega\in\Omega}\left\{\KeepStyleUnderBrace{\left[\mathds{1}_{\{y_{\omega}=1\}}-g_1(\theta^{\text{true}}_\omega)\right]
\frac{\dot{f}(b_1-\hat \theta_\omega)}{g_1(\hat \theta_\omega)}-\left[\mathds{1}_{\{y_{\omega}=2\}}-g_2(\theta^{\text{true}}_\omega)\right]
\frac{\dot{f}(b_1-\hat \theta_\omega)}{g_{2}(\hat \theta_\omega)}}_{:=W_\omega}\right\}.
\end{align}
Note that $\{W_\omega\}$ are i.i.d.\ random variables, and each $W_\omega$ has zero mean and bounded variance: 
\[
\text{Var}(W_\omega)\leq C\left(g_1(\theta_\omega^{\text{true}}) (1-g_1(\theta_\omega^{\text{true}}))+g_2(\theta_\omega^{\text{true}}) (1-g_2(\theta_\omega^{\text{true}}))\right)\leq {C/2}.
\]
By central limit theorem (verify..)
\[
\sum_{\omega\in\Omega}W_\omega \stackrel{\tD}{\rightarrow}N(0, {Cd^K}),\quad \text{as}\quad d^K\to\infty.
\]
Hence, with very high probability,
\begin{equation}\label{eq:A}
|A|=\left|\sum_{\omega\in\Omega}W_\omega \right|\leq Cd^{K/2} .
\end{equation}

The second term $B$ in~\eqref{eq:diff} is the bias induced by the inaccuracy of $\hat \Theta$:
\begin{align}
B&=\sum_{\omega \in \Omega}g_1(\theta^{\text{true}}_\omega) \left(  \frac{\dot{f}(b_1-\hat \theta_\omega)}{g_1(\hat \theta_\omega)}-\frac{\dot{f}(b_1-\theta^{\text{true}}_\omega)}{g_1(\theta^{\text{true}}_\omega)}\right)-\sum_{\omega \in \Omega}g_2(\theta^{\text{true}}_\omega) \left(  \frac{\dot{f}(b_2-\hat \theta_\omega)}{g_2(\hat \theta_\omega)}-\frac{\dot{f}(b_2-\theta^{\text{true}}_\omega)}{g_2(\theta^{\text{true}}_\omega)}\right) \notag\\
&\leq \sum_{\omega \in \Omega} g_1(\theta^{\text{true}}_\omega)  (\theta^{\text{true}}_\omega-\hat \theta_\omega )\left\{{\partial \over \partial \theta}\left({\dot{f}(b_1-\theta)\over g_1(\theta)}\right)\bigg|_{\rho \hat \theta_\omega+(1-\rho)\theta^{\text{true}}_\omega}\right\}\notag\\
&\quad - \sum_{\omega \in \Omega} g_2(\theta^{\text{true}}_\omega) (\theta^{\text{true}}_\omega-\hat \theta_\omega )  \left\{{\partial \over \partial \theta}\left({\dot{f}(b_2-\theta)\over g_2(\theta)}\right)\bigg|_{\rho' \hat \theta_\omega+(1-\rho')\theta^{\text{true}}_\omega}\right\} \notag\\
&\leq C\sum_{\omega\in\Omega} \left[g_1(\theta^{\text{true}}_\omega)- g_2(\theta^{\text{true}}_\omega)\right]\left(\theta^{\text{true}}_\omega-\hat \theta_\omega \right).
\end{align}
By Cauchy-Schwartz inequality,
\begin{equation}\label{eq:B}
|B|\leq Cd^{K/2}\FnormSize{}{\trueT-\hat \Theta}.
\end{equation}
Plugging \eqref{eq:A} and \eqref{eq:B} back to~\eqref{eq:diff} yields that
\[
\left|{\partial \tL_\tY\over \partial b_\ell}\Big|_{(\hat \Theta, \trueb)}\right|\leq C_1d^{K/2}+C_2d^{K/2}\FnormSize{}{\trueT-\hat \Theta}
\]
holds with very high probability. The second inequality in the conclusion comes from the fact that $\FnormSize{}{\trueT-\hat \Theta}\leq \tO(d)$ as $d\to \infty$. 
\end{proof}

\bibliography{tensor_wang}
\bibliographystyle{apalike}
\end{document}
