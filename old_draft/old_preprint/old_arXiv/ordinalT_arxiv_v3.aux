\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baltrunas2011incarmusic}
\citation{nickel2011three}
\citation{hore2016tensor}
\citation{zhou2013tensor}
\citation{hong2018generalized,zeng2019multiway}
\citation{ghadermarzy2018learning,ghadermarzy2019near}
\citation{anandkumar2014tensor,wang2017tensor}
\citation{hitchcock1927expression}
\citation{tucker1966some}
\citation{kolda2009tensor,ghadermarzy2019near}
\citation{ghadermarzy2018learning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\citation{cai2013max,davenport2014,bhaskar20151}
\citation{bhaskar2016probabilistic}
\citation{wang2018learning,hong2018generalized,ghadermarzy2018learning}
\citation{mccullagh1980regression}
\citation{ghadermarzy2018learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison with previous work when tensor rank $r=O(1)$. For ease of presentation, we summarize the error rate and sample complexity (neglecting log factors) when the tensor dimensions are equal in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode. }}{2}{table.1}}
\newlabel{tab:compare}{{1}{2}{Comparison with previous work when tensor rank $r=O(1)$. For ease of presentation, we summarize the error rate and sample complexity (neglecting log factors) when the tensor dimensions are equal in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{3}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model formulation and motivation}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation model}{3}{subsection.3.1}}
\newlabel{eq:model}{{3.1}{3}{Observation model}{subsection.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{eq:logodd}{{3.1}{3}{Observation model}{subsection.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\citation{mccullagh1980regression}
\citation{zhou2013tensor,bhaskar20151}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Latent-variable interpretation}{4}{subsection.3.2}}
\newlabel{sec:latent}{{3.2}{4}{Latent-variable interpretation}{subsection.3.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:quantization}{{3.2}{4}{Latent-variable interpretation}{subsection.3.2}{}}
\newlabel{eq:latent}{{3.2}{4}{Latent-variable interpretation}{subsection.3.2}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\newlabel{ass:link}{{1}{4}{}{assumption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Problem 1: Tensor denoising}{4}{subsection.3.3}}
\newlabel{sec:denoising}{{3.3}{4}{Problem 1: Tensor denoising}{subsection.3.3}{}}
\citation{kolda2009tensor}
\citation{hitchcock1927expression}
\citation{oseledets2011tensor}
\citation{de2008tensor}
\citation{negahban2011estimation,cai2013max,bhaskar20151}
\newlabel{eq:space}{{3.3}{5}{Problem 1: Tensor denoising}{subsection.3.3}{}}
\newlabel{eq:Tucker}{{3.3}{5}{Problem 1: Tensor denoising}{subsection.3.3}{}}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Problem 2: Tensor completion}{5}{subsection.3.4}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:space}
\@writefile{toc}{\contentsline {section}{\numberline {4}Rank-constrained M-estimator}{6}{section.4}}
\newlabel{sec:theory}{{4}{6}{Rank-constrained M-estimator}{section.4}{}}
\newlabel{eq:objective}{{4}{6}{Rank-constrained M-estimator}{section.4}{}}
\newlabel{eq:estimator}{{4}{6}{Rank-constrained M-estimator}{section.4}{}}
\newlabel{eq:regular}{{4}{6}{Rank-constrained M-estimator}{section.4}{}}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Estimation error for tensor denoising}{6}{subsection.4.1}}
\newlabel{sec:denosing}{{4.1}{6}{Estimation error for tensor denoising}{subsection.4.1}{}}
\MT@newlabel{eq:estimator}
\newlabel{thm:rate}{{4.1}{6}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:estimator}
\newlabel{eq:rate}{{4.1}{6}{Statistical convergence}{thm.4.1}{}}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:estimator}
\newlabel{cor:prediction}{{4.1}{7}{Prediction error}{cor.4.1}{}}
\newlabel{eq:KLrate}{{4.1}{7}{Prediction error}{cor.4.1}{}}
\MT@newlabel{eq:rate}
\newlabel{eq:ours}{{4.1}{7}{Estimation error for tensor denoising}{cor.4.1}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:KLrate}
\MT@newlabel{eq:estimator}
\newlabel{thm:minimax}{{4.2}{7}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:lower}{{4.2}{7}{Minimax lower bound}{thm.4.2}{}}
\citation{mu2014square}
\citation{yuan2016tensor}
\citation{zhang2019cross}
\citation{ghadermarzy2018learning}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:estimator}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sample complexity for tensor completion}{8}{subsection.4.2}}
\newlabel{eq:weighted}{{4.2}{8}{Sample complexity for tensor completion}{subsection.4.2}{}}
\MT@newlabel{eq:weighted}
\newlabel{thm:completion}{{4.3}{8}{}{thm.4.3}{}}
\MT@newlabel{eq:estimator}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Ordinal tensor decomposition}}{9}{algorithm.1}}
\newlabel{alg}{{1}{9}{Numerical Implementation}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Implementation}{9}{section.5}}
\newlabel{sec:algorithm}{{5}{9}{Numerical Implementation}{section.5}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:Tucker}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:model}
\newlabel{fig:stability}{{5}{10}{Numerical Implementation}{ALC@unique.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Trajectory of objective function with various $d$ and $r$.}}{10}{figure.1}}
\newlabel{eq:BIC}{{5}{10}{Numerical Implementation}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{10}{section.6}}
\newlabel{sec:experiment}{{6}{10}{Experiments}{section.6}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:Tucker}
\citation{davenport2014,sur2019modern}
\citation{acar2010scalable}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Finite-sample performance}{11}{subsection.6.1}}
\newlabel{sec:simulation}{{6.1}{11}{Finite-sample performance}{subsection.6.1}{}}
\MT@newlabel{eq:rate}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with alternative methods}{11}{subsection.6.2}}
\newlabel{sec:compare}{{6.2}{11}{Comparison with alternative methods}{subsection.6.2}{}}
\MT@newlabel{eq:model}
\citation{ghadermarzy2018learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\delimiter 69645069 \mathaccentV {hat}05E\Theta -\Theta ^{\mathrm  {true}}\delimiter 86422285 _F/\delimiter 69645069 \Theta ^{\mathrm  {true}}\delimiter 86422285 _F$ for better visualization.}}{12}{figure.2}}
\newlabel{fig:finite}{{2}{12}{Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\FnormSize {}{\hat \Theta -\trueT }/\FnormSize {}{\trueT }$ for better visualization}{figure.2}{}}
\MT@newlabel{eq:logodd}
\citation{van2013wu}
\citation{baltrunas2011incarmusic}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance comparison for predicting most likely labels. (a, c) Prediction error versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. (b, d) Prediction error versus the number of ordinal levels $L$ when $\rho =0.8$. }}{13}{figure.3}}
\newlabel{fig:compare}{{3}{13}{Performance comparison for predicting most likely labels. (a, c) Prediction error versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. (b, d) Prediction error versus the number of ordinal levels $L$ when $\rho =0.8$}{figure.3}{}}
\MT@newlabel{eq:latent}
\citation{stoeckel2009supramarginal}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance comparison for predicting median labels. (a, c) Prediction error versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. (b, d) Prediction error versus the number of ordinal levels $L$, when $\rho =0.8.$}}{14}{figure.4}}
\newlabel{fig:compare2}{{4}{14}{Performance comparison for predicting median labels. (a, c) Prediction error versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. (b, d) Prediction error versus the number of ordinal levels $L$, when $\rho =0.8.$}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Applications}{14}{section.7}}
\citation{baltrunas2011incarmusic}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses.}}{15}{table.2}}
\newlabel{table:CV}{{2}{15}{Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Proofs}{15}{section.8}}
\newlabel{sec:proof}{{8}{15}{Proofs}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Estimation error for tensor denoising}{15}{subsection.8.1}}
\newlabel{sec:proofMSE}{{8.1}{15}{Estimation error for tensor denoising}{subsection.8.1}{}}
\newlabel{eq:property}{{8.1}{15}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:property}
\newlabel{eq:bound}{{8.1}{15}{Estimation error for tensor denoising}{subsection.8.1}{}}
\newlabel{eq:taylor}{{8.1}{16}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:linear}{{8.1}{16}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:property}
\newlabel{eq:norm}{{8.1}{16}{Estimation error for tensor denoising}{subsection.8.1}{}}
\newlabel{eq:normrandom}{{8.1}{16}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:norm}
\MT@newlabel{eq:normrandom}
\newlabel{eq:linearconclusion}{{8.1}{16}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:quadratic}{{8.1}{16}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:taylor}
\MT@newlabel{eq:linearconclusion}
\MT@newlabel{eq:quadratic}
\newlabel{eq:KLbound1}{{8.1}{17}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:KLbound1}
\newlabel{eq:KLbound}{{8.1}{17}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:KLbound}
\newlabel{eq:totalKL}{{8.1}{17}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:totalKL}
\newlabel{eq:final}{{8.1}{17}{Estimation error for tensor denoising}{subsection.8.1}{}}
\MT@newlabel{eq:final}
\newlabel{eq:prob}{{8.1}{17}{Estimation error for tensor denoising}{subsection.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Sample complexity for tensor completion}{18}{subsection.8.2}}
\newlabel{eq:Taylor2}{{8.2}{18}{Sample complexity for tensor completion}{subsection.8.2}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:quadratic}
\newlabel{eq:linear2}{{8.2}{18}{Sample complexity for tensor completion}{Item.4}{}}
\newlabel{eq:quadratic2}{{8.2}{18}{Sample complexity for tensor completion}{Item.4}{}}
\MT@newlabel{eq:Taylor2}
\MT@newlabel{eq:quadratic2}
\newlabel{eq:sample}{{8.2}{18}{Sample complexity for tensor completion}{Item.4}{}}
\MT@newlabel{eq:sample}
\citation{brascamp2002extensions}
\citation{ghadermarzy2019near}
\citation{lim2005singular}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Convexity of the log-likelihood function}{19}{subsection.8.3}}
\newlabel{sec:proofconvexity}{{8.3}{19}{Convexity of the log-likelihood function}{subsection.8.3}{}}
\newlabel{thm:convexity}{{8.1}{19}{}{thm.8.1}{}}
\newlabel{eq:function}{{8.1}{19}{}{thm.8.1}{}}
\MT@newlabel{eq:function}
\newlabel{lem:lossconvexity}{{1}{19}{Corollary 3.5 in~\cite {brascamp2002extensions}}{lem.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Auxiliary lemmas}{19}{subsection.8.4}}
\newlabel{sec:lemma}{{8.4}{19}{Auxiliary lemmas}{subsection.8.4}{}}
\citation{friedland2018nuclear}
\citation{ghadermarzy2019near}
\citation{wang2017operator}
\citation{jiang2017tensor}
\citation{friedland2018nuclear}
\citation{nguyen2015tensor}
\citation{tomioka2014spectral}
\citation{tomioka2014spectral}
\newlabel{lem:Mnormbound}{{2}{20}{M-norm and infinity norm~\citep {ghadermarzy2019near}}{lem.2}{}}
\newlabel{lem:nuclear}{{3}{20}{Nuclear norm and F-norm}{lem.3}{}}
\newlabel{eq:norminequality}{{8.4}{20}{Auxiliary lemmas}{lem.3}{}}
\MT@newlabel{eq:norminequality}
\newlabel{lem:inq}{{4}{20}{}{lem.4}{}}
\newlabel{lem:tensor}{{5}{21}{Spectral norm of random tensors~\citep {tomioka2014spectral}}{lem.5}{}}
\newlabel{lem:noisytensor}{{6}{21}{}{lem.6}{}}
\newlabel{lem:KLentry}{{7}{21}{}{lem.7}{}}
\newlabel{eq:KL}{{8.4}{21}{Auxiliary lemmas}{lem.7}{}}
\newlabel{lem:KL}{{8}{22}{KL divergence and F-norm}{lem.8}{}}
\MT@newlabel{eq:model}
\newlabel{eq:ass}{{8}{22}{KL divergence and F-norm}{lem.8}{}}
\MT@newlabel{eq:model}
\newlabel{eq:entrywise}{{8.4}{22}{Auxiliary lemmas}{lem.8}{}}
\MT@newlabel{eq:ass}
\MT@newlabel{eq:entrywise}
\newlabel{lem:construction}{{9}{23}{}{lem.9}{}}
\newlabel{lem:VGbound}{{10}{23}{Varshamov-Gilbert bound}{lem.10}{}}
\citation{tsybakov2008introduction}
\citation{ghadermarzy2019near}
\citation{ge2017optimization,chen2019non}
\newlabel{lem:Tsybakov}{{11}{24}{Theorem 2.5 in~\cite {tsybakov2008introduction}}{lem.11}{}}
\newlabel{lem:convexity}{{12}{24}{Lemma 28 in~\cite {ghadermarzy2019near}}{lem.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{24}{section.9}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Extension of Theorem\nobreakspace  {}\ref  {thm:rate} to unknown cut-off points}{26}{appendix.A}}
\newlabel{sec:extention}{{A}{26}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{appendix.A}{}}
\newlabel{eq:joint}{{A}{26}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{appendix.A}{}}
\MT@newlabel{eq:bound}
\newlabel{eq:UL}{{4}{26}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{Item.12}{}}
\newlabel{eq:CD}{{5}{26}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{Item.13}{}}
\newlabel{ass:joint}{{2}{27}{}{assumption.2}{}}
\newlabel{thm:ratejoint}{{A.1}{27}{Statistical convergence with unknown $\mb $}{thm.A.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:joint}
\MT@newlabel{eq:UL}
\MT@newlabel{eq:CD}
\newlabel{eq:level}{{A}{28}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{rmk.6}{}}
\MT@newlabel{eq:level}
\newlabel{eq:ratenew}{{A}{28}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{rmk.6}{}}
\newlabel{eq:propertyb}{{A}{28}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{rmk.6}{}}
\newlabel{eq:taylorb}{{A}{28}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{rmk.6}{}}
\MT@newlabel{eq:taylorb}
\newlabel{eq:linearb}{{A}{28}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{rmk.6}{}}
\MT@newlabel{eq:taylorb}
\newlabel{eq:quadraticb}{{A}{28}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{rmk.6}{}}
\MT@newlabel{eq:taylorb}
\MT@newlabel{eq:linearb}
\MT@newlabel{eq:quadraticb}
\newlabel{lem:clt}{{13}{30}{CLT for i.d. Bernoulli}{lem.13}{}}
\newlabel{lem:sharpergd}{{14}{30}{Bound on Gradients}{lem.14}{}}
\newlabel{eq:sharpergd}{{A}{31}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{lem.14}{}}
\MT@newlabel{eq:sharpergd}
\MT@newlabel{eq:sharpergd}
\newlabel{eq:A}{{A}{31}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{lem.14}{}}
\MT@newlabel{eq:sharpergd}
\newlabel{eq:B}{{A}{31}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{lem.14}{}}
\MT@newlabel{eq:A}
\MT@newlabel{eq:B}
\MT@newlabel{eq:sharpergd}
\citation{kolda2009tensor}
\citation{stoeckel2009supramarginal}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional results of HCP analysis}{32}{appendix.B}}
\newlabel{sec:additionalHCP}{{B}{32}{Additional results of HCP analysis}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Clustering based on Tucker representation}{32}{subsection.B.1}}
\newlabel{sec:clustering}{{B.1}{32}{Clustering based on Tucker representation}{subsection.B.1}{}}
\newlabel{eq:Tuckerest}{{B.1}{32}{Clustering based on Tucker representation}{subsection.B.1}{}}
\MT@newlabel{eq:Tuckerest}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Clustering results of HCP analysis}{32}{subsection.B.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces Elbow plot for determining the number of clusters in $K$-means.}}{33}{figure.1}}
\newlabel{figure:elbow}{{S1}{33}{Elbow plot for determining the number of clusters in $K$-means}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces Cluster image of brain nodes when the number of clusters is 8 (a,b), 11 (c,d) and 13 (e,f). (a,c,e) Nodes from the same cluster are colored with the same color. (b,d,f) Predicted connectivity within clusters. Edges are colored based on predicted strength level averaged across individuals.}}{34}{figure.2}}
\newlabel{figure:brain image}{{S2}{34}{Cluster image of brain nodes when the number of clusters is 8 (a,b), 11 (c,d) and 13 (e,f). (a,c,e) Nodes from the same cluster are colored with the same color. (b,d,f) Predicted connectivity within clusters. Edges are colored based on predicted strength level averaged across individuals}{figure.2}{}}
\bibdata{tensor_wang}
\bibstyle{apalike}
\@writefile{lot}{\contentsline {table}{\numberline {S1}{\ignorespaces Clustering result of brain nodes. The first alphabet in the node name indicates the left (L) or right (R) hemisphere. The number in the parentheses indicates the node count in each cluster. }}{35}{table.1}}
\newlabel{table:clustering}{{S1}{35}{Clustering result of brain nodes. The first alphabet in the node name indicates the left (L) or right (R) hemisphere. The number in the parentheses indicates the node count in each cluster}{table.1}{}}
