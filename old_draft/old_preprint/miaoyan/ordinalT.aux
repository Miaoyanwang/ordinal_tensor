\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{eq:model}{{1}{1}{}{equation.3.1}{}}
\MT@newlabel{eq:model}
\citation{mccullagh1980regression}
\citation{zhou2013tensor,bhaskar20151}
\newlabel{tab:compare}{{1}{2}{Comparison of various low-rank estimation methods based on categorical observations. For an order-$K$ dimensional-$(d,\ldots ,d)$ ordinal tensor with $L$-level observations, we report the error bound in the recovered signal (for tensor denoising) and required sample complexity (for tensor completion) as functions of tensor dimensions. {\color {red}(font in the table seems odd...) Add a row ``algorithm stability''}}{table.1}{}}
\MT@newlabel{eq:model}
\newlabel{eq:logodd}{{2}{2}{}{equation.3.2}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\newlabel{sec:latent}{{3.2}{2}{}{subsection.3.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:quantization}{{3}{2}{}{equation.3.3}{}}
\newlabel{eq:latent}{{4}{2}{}{equation.3.4}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\newlabel{ass:link}{{1}{2}{}{assumption.1}{}}
\newlabel{sec:denoising}{{3.3}{2}{}{subsection.3.3}{}}
\citation{kolda2009tensor}
\citation{hitchcock1927expression}
\citation{oseledets2011tensor}
\citation{de2008tensor}
\citation{negahban2011estimation,cai2013max,bhaskar20151}
\newlabel{eq:space}{{5}{3}{}{equation.3.5}{}}
\newlabel{eq:Tucker}{{6}{3}{}{equation.3.6}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:model}
\MT@newlabel{eq:space}
\newlabel{eq:objective}{{7}{3}{}{equation.4.7}{}}
\newlabel{eq:estimator}{{8}{3}{}{equation.4.8}{}}
\newlabel{eq:regular}{{4}{3}{}{equation.4.8}{}}
\MT@newlabel{eq:latent}
\newlabel{sec:denosing}{{4.1}{3}{}{subsection.4.1}{}}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\MT@newlabel{eq:estimator}
\newlabel{thm:rate}{{4.1}{4}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:estimator}
\newlabel{eq:rate}{{9}{4}{Statistical convergence}{equation.4.9}{}}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:estimator}
\newlabel{cor:prediction}{{1}{4}{Prediction error}{cor.1}{}}
\newlabel{eq:KLrate}{{10}{4}{Prediction error}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\newlabel{eq:ours}{{4.1}{4}{}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:KLrate}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:rate}
\newlabel{thm:minimax}{{4.2}{4}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:lower}{{4.2}{4}{Minimax lower bound}{thm.4.2}{}}
\newlabel{eq:weighted}{{11}{4}{}{equation.4.11}{}}
\MT@newlabel{eq:weighted}
\citation{mccullagh1980regression}
\citation{kolda2008scalable,anandkumar2014tensor}
\newlabel{alg}{{1}{5}{}{algorithm.1}{}}
\newlabel{thm:completion}{{4.3}{5}{}{thm.4.3}{}}
\MT@newlabel{eq:estimator}
\newlabel{sec:algorithm}{{5}{5}{}{section.5}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:Tucker}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:model}
\newlabel{fig:stability}{{5}{5}{}{algorithm.1}{}}
\citation{davenport2014,sur2018modern}
\citation{filipovic2015tucker}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\newlabel{eq:BIC}{{5}{6}{}{figure.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:Tucker}
\newlabel{sec:simulation}{{6.1}{6}{}{subsection.6.1}{}}
\MT@newlabel{eq:rate}
\newlabel{fig:finite}{{2}{6}{Finite sample performance}{figure.2}{}}
\citation{filipovic2015tucker}
\citation{bhaskar2016probabilistic}
\bibdata{tensor_wang}
\bibcite{anandkumar2014tensor}{{1}{2014}{{Anandkumar et~al.}}{{Anandkumar, Ge, Hsu, Kakade, and Telgarsky}}}
\bibcite{bhaskar2016probabilistic}{{2}{2016}{{Bhaskar}}{{}}}
\bibcite{bhaskar20151}{{3}{2015}{{Bhaskar \& Javanmard}}{{Bhaskar and Javanmard}}}
\newlabel{CV}{{7.1}{7}{}{subsection.7.1}{}}
\MT@newlabel{eq:model}
\bibcite{cai2013max}{{4}{2013}{{Cai \& Zhou}}{{Cai and Zhou}}}
\bibcite{davenport2014}{{5}{2014}{{Davenport et~al.}}{{Davenport, Plan, Van Den~Berg, and Wootters}}}
\bibcite{de2008tensor}{{6}{2008}{{De~Silva \& Lim}}{{De~Silva and Lim}}}
\bibcite{filipovic2015tucker}{{7}{2015}{{Filipovi{\'c} \& Juki{\'c}}}{{Filipovi{\'c} and Juki{\'c}}}}
\bibcite{ghadermarzy2018learning}{{8}{2018}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{hitchcock1927expression}{{9}{1927}{{Hitchcock}}{{}}}
\bibcite{kolda2009tensor}{{10}{2009}{{Kolda \& Bader}}{{Kolda and Bader}}}
\bibcite{kolda2008scalable}{{11}{2008}{{Kolda \& Sun}}{{Kolda and Sun}}}
\bibcite{mccullagh1980regression}{{12}{1980}{{McCullagh}}{{}}}
\bibcite{negahban2011estimation}{{13}{2011}{{Negahban et~al.}}{{Negahban, Wainwright, et~al.}}}
\bibcite{oseledets2011tensor}{{14}{2011}{{Oseledets}}{{}}}
\bibcite{sur2018modern}{{15}{2018}{{Sur \& Cand{\`e}s}}{{Sur and Cand{\`e}s}}}
\bibcite{zhou2013tensor}{{16}{2013}{{Zhou et~al.}}{{Zhou, Li, and Zhu}}}
\bibstyle{icml2020}
