\begin{thebibliography}{16}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anandkumar et~al.(2014)Anandkumar, Ge, Hsu, Kakade, and
  Telgarsky]{anandkumar2014tensor}
Anandkumar, A., Ge, R., Hsu, D., Kakade, S.~M., and Telgarsky, M.
\newblock Tensor decompositions for learning latent variable models.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 2773--2832, 2014.

\bibitem[Bhaskar(2016)]{bhaskar2016probabilistic}
Bhaskar, S.~A.
\newblock Probabilistic low-rank matrix completion from quantized measurements.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2131--2164, 2016.

\bibitem[Bhaskar \& Javanmard(2015)Bhaskar and Javanmard]{bhaskar20151}
Bhaskar, S.~A. and Javanmard, A.
\newblock 1-bit matrix completion under exact low-rank constraint.
\newblock In \emph{Information Sciences and Systems (CISS), 2015 49th Annual
  Conference on}, pp.\  1--6. IEEE, 2015.

\bibitem[Cai \& Zhou(2013)Cai and Zhou]{cai2013max}
Cai, T. and Zhou, W.-X.
\newblock A max-norm constrained minimization approach to 1-bit matrix
  completion.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 3619--3647, 2013.

\bibitem[Davenport et~al.(2014)Davenport, Plan, Van Den~Berg, and
  Wootters]{davenport2014}
Davenport, M.~A., Plan, Y., Van Den~Berg, E., and Wootters, M.
\newblock 1-bit matrix completion.
\newblock \emph{Information and Inference: A Journal of the IMA}, 3\penalty0
  (3):\penalty0 189--223, 2014.

\bibitem[De~Silva \& Lim(2008)De~Silva and Lim]{de2008tensor}
De~Silva, V. and Lim, L.-H.
\newblock Tensor rank and the ill-posedness of the best low-rank approximation
  problem.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 30\penalty0
  (3):\penalty0 1084--1127, 2008.

\bibitem[Filipovi{\'c} \& Juki{\'c}(2015)Filipovi{\'c} and
  Juki{\'c}]{filipovic2015tucker}
Filipovi{\'c}, M. and Juki{\'c}, A.
\newblock Tucker factorization with missing data with application to low-rank
  tensor completion.
\newblock \emph{Multidimensional systems and signal processing}, 26\penalty0
  (3):\penalty0 677--692, 2015.

\bibitem[Ghadermarzy et~al.(2018)Ghadermarzy, Plan, and
  Yilmaz]{ghadermarzy2018learning}
Ghadermarzy, N., Plan, Y., and Yilmaz, O.
\newblock Learning tensors from partial binary measurements.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (1):\penalty0 29--40, 2018.

\bibitem[Hitchcock(1927)]{hitchcock1927expression}
Hitchcock, F.~L.
\newblock The expression of a tensor or a polyadic as a sum of products.
\newblock \emph{Journal of Mathematics and Physics}, 6\penalty0 (1-4):\penalty0
  164--189, 1927.

\bibitem[Kolda \& Bader(2009)Kolda and Bader]{kolda2009tensor}
Kolda, T.~G. and Bader, B.~W.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[Kolda \& Sun(2008)Kolda and Sun]{kolda2008scalable}
Kolda, T.~G. and Sun, J.
\newblock Scalable tensor decompositions for multi-aspect data mining.
\newblock In \emph{2008 Eighth IEEE international conference on data mining},
  pp.\  363--372. IEEE, 2008.

\bibitem[McCullagh(1980)]{mccullagh1980regression}
McCullagh, P.
\newblock Regression models for ordinal data.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 42\penalty0 (2):\penalty0 109--127, 1980.

\bibitem[Negahban et~al.(2011)Negahban, Wainwright,
  et~al.]{negahban2011estimation}
Negahban, S., Wainwright, M.~J., et~al.
\newblock Estimation of (near) low-rank matrices with noise and
  high-dimensional scaling.
\newblock \emph{The Annals of Statistics}, 39\penalty0 (2):\penalty0
  1069--1097, 2011.

\bibitem[Oseledets(2011)]{oseledets2011tensor}
Oseledets, I.~V.
\newblock Tensor-train decomposition.
\newblock \emph{SIAM Journal on Scientific Computing}, 33\penalty0
  (5):\penalty0 2295--2317, 2011.

\bibitem[Sur \& Cand{\`e}s(2018)Sur and Cand{\`e}s]{sur2018modern}
Sur, P. and Cand{\`e}s, E.~J.
\newblock A modern maximum-likelihood theory for high-dimensional logistic
  regression.
\newblock \emph{arXiv preprint arXiv:1803.06964}, 2018.

\bibitem[Zhou et~al.(2013)Zhou, Li, and Zhu]{zhou2013tensor}
Zhou, H., Li, L., and Zhu, H.
\newblock Tensor regression with applications in neuroimaging data analysis.
\newblock \emph{Journal of the American Statistical Association}, 108\penalty0
  (502):\penalty0 540--552, 2013.

\end{thebibliography}
