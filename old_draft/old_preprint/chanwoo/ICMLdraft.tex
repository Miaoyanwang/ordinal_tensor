b%%%%%%%% ICML 2020 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2020}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2020}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Ordinal tensor denoising and completion}

\usepackage{multirow}
\usepackage{graphicx}
%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath,amssymb}
\usepackage{amsthm}    % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{bm}
%\usepackage{subfig}
%\usepackage[english]{babel}
%\usepackage{algorithm}
%\usepackage{appendix}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}

\usepackage{dsfont}
%\usepackage{algpseudocode,algorithm}
%\algnewcommand\algorithmicinput{\textbf{Input:}}
%\algnewcommand\algorithmicoutput{\textbf{Output:}}
%\algnewcommand\INPUT{\item[\algorithmicinput]}
%\algnewcommand\OUTPUT{\item[\algorithmicoutput]}
%\DeclareMathOperator*{\minimize}{minimize}




\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}

\input macros.tex

\begin{document}

\twocolumn[
\icmltitle{Tensor denoising and completion based on ordinal observations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2020
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chanwoo Lee}{equal,to}
\icmlauthor{Miaoyan Wang}{equal,to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Statistics, University of Wisconsin at Madison, USA}

\icmlcorrespondingauthor{Miaoyan Wang}{miaoyan.wang@wisc.edu.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Higher-order tensors, ordinal data, tensor denoising, tensor completion}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}


\end{abstract}


\section{Introduction}
Multidimensional array datasets, a.k.a. a tensor, appear in a huge variety of applications including recommendation systems \citep{kutty2012people,adomavicius2011context,sun2017provable},
social networks \citep{sun2009multivis,nickel2011three},
genomics \citep{wang2018learning},
neuroimaging (EEG, fMRI) \citep{miwakeichi2004decomposing}
and signalprocessing \citep{sidiropoulos2000parallel,cichocki2015tensor}.
Instead of unfolding those data tensors into matrices where many analysis methods have been proposed, we suggest preserving multi-modal tensor structure in this paper. Studying tensor data while respecting the structure allows us to examine complex interactions among tensor entries. Thereby we can expect to provide extra more interpretation that cannot be addressed by traditional matrix analysis. Furthermore, It has been shown that the tensor preserving analysis improves performance \citep{zare2018extension,wang2018learning}.
With those reasons, there is a growing need to develop dimension reduction method without losing tensor structure. In the line of the attempts, a number of tensor decomposition methods have been proposed in many applications. CANDECOMP/PARAFAC (CP) decomposition was first introduced \citep{hitchcock1928multiple}
and made use of in psychometrics \citep{harshman1970foundations} and in linguistics \citep{smilde2005multi}. The Tucker decomposition was proposed in psychometrics \citep{tucker1964extension,tucker1966some}.

Classical tensor completion with those decompositions has treated the entries of data as real-valued.
In many cases, However, we encounter data sets of which the entries are not real-valued but discrete or quantized i.e. binary-valued or ordinal-valued. For example, many survey data sets take the integer values. To be specific, the data set in the Netflix problem has the three modes of `user',`movie' and `date of grade'. The entries of the data are the ratings from the users which take integer value from 1 to 5. Another example is the case where the data sets are quantized in real application. In signal processing, the data sets are frequently rounded or truncated so that only integer values are available. In network analysis, entries of adjacency matrix can be labeled as from 1 to 3 taking 3 when pairs of vertexes have strong connection and giving 1 when two vertexes have the weak connection according to a given threshold. If we add one more mode on adjacency matrix such as `context' or `individual', the data turns into tensor data with 3 integer values.

We expect that performance improvement can be achieved when the observations are treated as discrete ordinal value not a continuous value.
In matrix case, there has been many achievements to complete matrix for discrete cases: Models for the case of binary or 1-bit were introduced and studied \citep{davenport20141,bhaskar20151}. Furthermore,  \citet{bhaskar2016probabilistic} suggested matrix completion method for general ordinal observations.
In tensor case, however, only binary tensor has gained an attention and achieved performance improvement using binary tensor decomposition methods \citep{hore2016tensor,wang2018learning,hong2018generalized,hu2018training}. Accordingly, a general method for the data which has more than 2 ordered label is needed.

We organized this paper as follows. We specify our notations that we will use over this paper in the next section.
In section 3, we discuss detailed assumptions and descriptions about our probabilistic model. Also, we suggest the estimation method for the latent parameters and related algorithm.
In section 4, we provide the statistical properties of the upper, lower bounds and the phase-transition.
We then provide the numerical experiments.
Our model is applied to  real-world data to check validity and performance in section 6.
Finally, we wrap up the paper with a discussion.

\section{Preliminaries}
Let $\tY\in\mathbb{R}^{d_1\times \cdots \times d_K}$ denote an order-$K$ $(d_1,\ldots,d_K)$-dimensional tensor. We use $y_\omega$ to denote the tensor entry indexed by $\omega$, where $\omega\in[d_1]\times\cdots\times[d_K]$.  The Frobenius norm of $\tY$ is defined as $\FnormSize{}{\tY}=\sum_\omega y^2_\omega$ and the infinity norm of $\tY$ is defined as $\mnormSize{}{\tY}=\max_{\omega}|y_\omega|$. We use $\tY_{(k)}$ to denote the unfolded matrix of size $d_k$-by-$\prod_{i\neq k}d_k$, obtained by reshaping the tensor along the mode-$k$. The Tucker rank of $\tY$ is defined as a length-$K$ vector $\mr = (r_1,\ldots,r_K)$, where $r_k$ is the rank of matrix $\tY_{(k)}$ for all $k \in[L]$. We say that an event $A$ occurs ``with very high probability'' if $\mathbb{P}(A)$ tends to 1 faster than any polynomial of tensor dimension $d_{\min}=\min\{d_1,\ldots,d_K\}$ as $d_{\min}\to\infty$.

We use lower-case letters (e.g., $a$, $b$, $c$) for scalars/vectors, upper-case boldface letters (e.g., $\mA$, $\mB$, $\mC$) for matrices, and calligraphy letters (e.g., $\tA$, $\tB$, $\tC$) for tensors of order three or greater. For ease of notation, we allow the basic arithmetic operators (e.g., $\leq, +, -$) to be applied to pairs of vectors in an element-wise manner. We use the shorthand $[n]$ to denote the $n$-set $\{1,\ldots,n\}$ for $n \in N_{+}$.



\section{Model}
\subsection{Low-rank ordinal tensor model}
For the $K$-mode ordinal tensor $\mathcal{Y} = [\![ y_\omega]\!]\in[L]^{d_1\times\cdots\times d_K}$, we assume that its entries are realizations of independent multinomial random variables, such that
\begin{equation}
    \begin{aligned}
        \label{mainmodel}
        \mathbb{P}(y_\omega\leq \ell) = f(\theta_\omega+b_\ell), \quad \omega\in [d_1]\times\cdots\times[d_K].
    \end{aligned}
\end{equation}
In this model, a twice differentiable function $f: \mathbb{R}\rightarrow [0,1]$  is known and a cumulative distribution function and $\mb = (b_1,\cdots,b_{L-1})$ is a set of unknown scalars satisfying $b_1<\cdots<b_{L-1}$. We refer to $\mb$ as the cut-off points and $f$ the link function.
The tensor $\Theta = [\![\theta_\omega]\!]\in \mathbb{R}^{d_1\times\cdots\times d_K}$
is a hidden parameter which we are interested in. We assume that the entries of the parameter tensor $\Theta$ are continuous valued and $\Theta$ admits a rank $\mr = (r_1,\cdots,r_K)$ Tucker decomposition,
\begin{equation}
    \begin{aligned}
        \label{tucker}
        \Theta = \mathcal{C}\times_1\mM_1\cdots\times_K\mM_N,
    \end{aligned}
\end{equation}
where $\mathcal{C}\in \mathbb{R}^{r_1\times\cdots\times r_K}$ is a full rank core tensor and $\mM_k\in \mathbb{R}^{d_k\times r_k}$, for $n \in [L]$ is a factor matrix with orthogonal columns. This low rank tensor structure allows us to reduce dimension and complete tensors from available entries.

The ordinal tensor model \eqref{mainmodel} has the equivalent representation as a latent model with $L$-level quantization. \citep{davenport20141,lan2014matrix,bhaskar20151,cai2013max} where the entry of
$\mathcal{Y} = [\![ y_\omega]\!]$ is a quantized value such that,
\begin{equation}
    \label{latentmd}
\begin{aligned}
    y_\omega = \mathcal{Q}(\theta_\omega+\epsilon_\omega),\quad \omega\in [d_1]\times\cdots[d_K],
\end{aligned}
\end{equation}
where $\mathcal{E} = [\![\epsilon_\omega]\!]$ is a noise tensor i.i.d. entries from cumulative distribution function $\mathcal{F}_\epsilon$ such that $f(\theta) = \mathbb{P}(\epsilon \geq -\theta)$ and  $\mathcal{Q}:\mathbb{R}\rightarrow [L]$ is a quantizer having the following rule.
\begin{equation}
    \label{qfunction}
    \begin{aligned}
        \mathcal{Q}(x) = \ell,\quad \text{ if } b_{\ell-1}< x \leq b_{\ell},\quad \text{ for all } \ell\in [L],
    \end{aligned}
\end{equation}
That is, the entries of observed tensor $\mathcal{Y}$ fall into category $\ell$ when the associated entries of the latent tensor $\Theta+\mathcal{E}$ fall into the $\ell$-th interval of values. From this point of view, we can see the latent parameter $\Theta$ as the signal prior to contamination and quantization.
We can diversify our model by the choices of $f$, or equivalently the distribution of $\mathcal{E}$.
The followings are two common choices of $f$.

\begin{example}[Logistic link/Logistic noise] The logistic model is represented by \eqref{mainmodel} with $f(\theta) = \Phi_{log}(\theta/\sigma)$  where $\Phi_{log}(x/\sigma) = (1+e^{-x/\sigma})$. Equivalently, the noise $\epsilon_\omega$ in \eqref{latentmd} follows i.i.d. logistic distribution with the scale parameter $\sigma.$
\end{example}

\begin{example}[Probit link/Gaussian noise] The probit model is represented by \eqref{mainmodel} with $f(\theta) = \Phi_{norm}(\theta/\sigma)
$ where $\Phi_{norm}$ is the cumulative distribution function of the standard Gaussian. Equivalently, the noise $\epsilon_\omega$ in \eqref{latentmd} follows i.i.d.$N(0,\sigma^2)$.
\end{example}

\subsection{Rank-constrained likelihood-based estimation}
Our goal is to estimate unknown parameter tensor $\Theta$ and cut-off points $\mb$ from observed tensor $\mathcal{Y}$ using a constrained likelihood approach. With a little abuse of notation, we use $\Omega$ to denote either the full index set $\Omega = [d_1]\times\cdots\times[d_K]$ or a random subset induced from the subsampling distribution. The log-likelihood function for \eqref{mainmodel} is
\begin{equation}
    \label{likelihood}
    \begin{aligned}
    \mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb) = \sum_{\omega\in\Omega}\Big[\sum_{\ell\in[L]}\log(g_\ell(\theta_\omega))\mathds{1}_{\{y_\omega = \ell\}}\Big],
    \end{aligned}
\end{equation}
where $g_\ell(x) = \mathbb{P}(y_\omega = \ell) = f(\theta_\omega+b_\ell)-f(\theta_\omega+b_{\ell-1})$ from \eqref{mainmodel}. We define $b_0 = -\infty,b_L = \infty$ so that $f(b_0) = 0, f(b_L) = 1$. The cut-off points $\mb$ is implicitly contained in the function $g_\ell$.
Considering the Tucker structure in \eqref{tucker}, we have the following constrained optimization problem.
\begin{equation}
    \label{eq:optimization}
    \begin{aligned}
        &\max_{(\Theta, \mb)\in \mathcal{D}\times\mathcal{B}}\mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb),\text{ where }\\
        &\mathcal{D} = \{\Theta\in \mathbb{R}^{d_1\times\cdots\times d_K}: \rank(\Theta) \leq \bm{r} \text{ and } \mnormSize{}{\Theta}\leq \alpha \}\\
        &\mathcal{B} = \{\mb\in\mathbb{R}^{L-1}:  b_1<\cdots < b_{L-1}\},
    \end{aligned}
\end{equation}
for a given rank $\bm r\in \mathbb{N_+}^K$ and a bound $\alpha \in \mathbb{R}_+$. The search space $\mathcal{D}$ has two constraints on unknown parameter $\Theta$. The first constraint ensures that the unknown parameter $\Theta$ admits the Tucker decomposition with rank $\bm r$. The second constraint makes the entries of $\Theta$ bounded by a constant $\alpha$. This bound condition is a technical assumption to help to recover $\Theta$ in the noiseless case. Similar conditions has been imposed in many literatures for the matrix case \citep{davenport20141,bhaskar20151,cai2013max,bhaskar2016probabilistic}.
The search space $\mathcal{B}$ makes sure that the probability function $g_\ell$ in \eqref{likelihood} is strictly positive.

\subsection{Optimization}
In this section, we describe the algorithm to seek the optimizer of \eqref{eq:optimization}. The objective function $\mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb)$ is concave in $(\Theta,\mb)$ whenever $f(x)$ is log-concave \citep{mccullagh1980regression,burridge1981note}.
However, the feasible set $\mathcal{D}$ is not a convex set, which makes the optimization \eqref{eq:optimization} a non-convex problem. One approach to handle this problem is utilizing the Tucker decomposition and converting optimization into a block-wise convex problem.
From \eqref{likelihood} and \eqref{tucker}, we have $K+2$ blocks of variables in the objective function, one for the cut-points vector $\mb$, one for the core tensor $\mathcal{C}$ and  $K$ for the factor matrices $\mM_K$'s.
We can change the optimization problem to simple convex problem if any $K+1$ out of the $K+2$ blocks being fixed. Therefore, we can alternately update one block at a time while other blocks being fixed.
The algorithm \ref{alg} gives the full description.



\begin{algorithm}[]
        \caption{Ordinal tensor decomposition }\label{alg}
        \begin{algorithmic}[]
            \STATE{\bfseries Input:}  \text{ Ordinal tensor }
            $\mathcal{Y}\in [L]^{d_1\times\cdots\times d_K}
            $,\\ \hspace{.43in} Rank $\mr\in \mathbb{N_+}^{K-1}$,\\
            \hspace{.43in} Entry-wise bound $\alpha\in \mathbb{R_+}$.
            \STATE{\bfseries Output:} $(\hat\Theta,\hat{\mb}) =  \argmax_{(\Theta,\mb)\in \mathcal{D}\times\mathcal{B}}  \mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb).$
            \STATE Initialize  Core tensor $\mathcal{C}^{(0)}$,\\
            \hspace{.5in} Factor matrices $ \{\mM_1^{(0)},\cdots,\mM_K^{(0)}\}$,\\
            \hspace{.5in} Cut-off points $\mb^{(0)}$.
            \FOR{t = 1,2,$\cdots$,}
            \FOR{k = 1,2,$\cdots$,K}
            \STATE{Update $\mM_k$ while fixing other blocks:}
            \STATE $\mM_k^{(t+1)}\gets\argmax_{\mM_k\in\mathbb{R}^{d_k\times r_k}}\mathcal{L}_{\mathcal{Y},\Omega}(\mM_k)$
            \newline
             s.t. $\mnormSize{}{\Theta^{(t+1)}}\leq \alpha$, where $\Theta^{(t+1)}$ is the parameter tensor based on the current block estimates.
            \ENDFOR
            \STATE {Update $\mathcal{C}$ while fixing other blocks:}
            \STATE $\mathcal{C}^{(t+1)} \gets \argmax_{\mathcal{C}\in \mathbb{R}^{r_1\times\cdots\times r_k}}\mathcal{L}_{\mathcal{Y},\Omega}(\mathcal{C})$
            s.t.  $\mnormSize{}{\Theta^{(t+1)}}\leq \alpha$
            \STATE Update $\Theta$ based on the current block estimates:
            \STATE $\Theta^{(t+1)} \gets \mathcal{C}^{(t+1)}\times_1\mM_1^{(t+1)}\cdots\times_K\mM_K^{(t+1)}$
            \STATE {Update $\mb$ while fixing $\Theta^{(t+1)}$:}
            \STATE $\mb^{(t+1)} \gets \argmax_{\mb\in \mathbb{R}^{L-1}}\mathcal{L}_{\mathcal{Y},\Omega}\big(\mb\big)$
            \ENDFOR
            \STATE \textbf{return}
            $\Theta,\mb$
        \end{algorithmic}
    \end{algorithm}



\subsection{Rank selection}
Algorithm \ref{alg} takes  the rank $\bm r$ as an input variable. In practice, the rank $\bm r$ is hardly known. Estimating an appropriate rank $\bm r$ from a given tensor is an important issue. We suggest to use Bayesian Information Criterion(BIC) to choose the rank.
\begin{equation}
    \label{bic}
    \begin{aligned}
        \hat{\bm r} &= \argmin_{\bm r\in \mathbb{N}_+^K} BIC(\bm r)\\
        &=\argmin_{\bm r\in \mathbb{N}_+^K}\bigg[-2\mathcal{L}_{\mathcal{Y},\Omega}(\hat\Theta(\bm r),\hat\mb(\mr)) +
        p_e(\mr)\log\big(\prod_{k\in[K]} d_k\big)\bigg],
    \end{aligned}
\end{equation}
where $\hat\Theta(\bm r),\hat\mb(\mr)$ is a maximum likelihood estimate given the rank $\bm r$, and $p_e(\rm)\eqdef \sum_k(d_k-r_k)r_k = \prod_k r_k$ is the effective number of free parameters in the model. We select the rank $\hat{\bm r}$ that minimizes BIC value through the grid search method.
\section{Real-world Data Applications}
In this section, we apply our ordinal tensor decomposition method to two real-world datasets of ordinal tensors. In the first application, we use our model to analyze an ordinal tensor consisting of structural connectivity patterns among 68 brain regions for 136 individuals from Human Connectome Project (HCP) \citep{geddes2016human}. In the second application, we perform tensor completion from the data with missing values. The data tensor records the ratings from scale 1 to 5 of 42 users to 139 songs on 26 contexts \citep{baltrunas2011incarmusic}.
\subsection{Human Connectome Project (HCP)}
The human connectome project (HCP) is a $68\times 68 \times 136$ tensor where the first two modes have 68 indices representing brain regions and the last mode has 136 indices meaning individuals. All the individual images were preprocessed following a standard pipeline \citep{zhang2018mapping}, and the brain was parcellated to 68 regions of interest following the Desikan atlas \citep{desikan2006automated}. The tensor entries consist of $\{1,2,3\}$, the strength of fiber connections between 68 brain regions for each of the 136 individuals. We apply our ordinal tensor decomposition method with a logistic link function to the HCP data. The BIC result suggests $\bm r = (23,23,8)$  with $\mathcal{L}_\mathcal{Y}(\hat{\Theta},\hat{\mb}) = -216645.8$.
First, we estimate latent parameters $\Theta$ from the full dataset and cluster the brain nodes based on $K$-means method from the estimated latent parameters. We decide the number of clusters as eight from the elbow method. The cluster result shows that most of the brain nodes fall into two biggest groups which can be represented as right-side brain and left-side brain.
The brain nodes, in each group except the above two biggest clusters, have the similar names and belong to the same location(right or left). For example, the fourth cluster group has the nodes having names such as \textit{l.postcentral, l.posteriorcingulate, l.pericalcarine and l.temporalpole,} all of which are encoded as ``SupraM$\_$L'' in the dataset. The brain clustering based on the latent estimator successfully catches regional traits of the brain nodes without external knowledge. More detailed procedure and result are provided in the supplements.

Secondly, we check the performance of our model and compare with other methods: the continuous tensor decomposition method\cite{filipovic2015tucker}, the 1 bit-tensor completion method\cite{ghadermarzy2018learning}. In 1 bit-tensor completion method, we use the probit link instead of the logistic link because the probit link has the better performance. We use stratified 5-folded cross validation method to assess the performances. Specifically, we randomly split the tensor entries into five pieces with the same size making sure that every piece has the similar proportion of ordinal observations.
We alternately use each piece of entries as a test dataset and use the other $80\%$ entries as a training dataset. The entries of the test dataset are encoded as missing and then predicted based on each method from the training dataset. In our model, We suggest three estimators $\hat y_\omega$ based on $(\hat \theta_\omega,\hat \mb)$:
\begin{itemize}
\item (Mean) $\hat y^{(\text{Mean})}_\omega=\sum_\ell \ell g_\ell(\hat \theta_\omega,\hat \mb) $;
\item (Median) $\hat y^{(\text{Median})}_\omega=\min\{\ell\in[L]\colon f_\ell(\hat \theta_\omega,\hat \mb)\geq 0.5\}$;
\item (Mode) $\hat y^{(\text{Mode})}_\omega=\argmax_\ell g_\ell(\hat \theta_\omega,\hat \mb) $
\end{itemize}
Notice that, under the ordinal tensor model~\eqref{mainmodel}, the estimator $\hat y^{(\text{Mean})}_\omega$ minimizes $\mathbb{E}_{\hat{\theta}_\omega,\hat{\mb}}(y_\omega-y)^2$, the estimator $\hat y^{(\text{Median})}_\omega$ minimizes $\mathbb{E}_{\hat{\theta}_\omega,\hat{\mb}}|y_\omega-y|$
and the estimator $\hat y^{(\text{Mode})}_\omega $ minimizes $\mathbb{E}_{\hat{\theta}_\omega,\hat{\mb}}\mathds{1}_{\{y_\omega=y\}}$.
We assess the accuracy using two metrics: Mean absolute deviation (MAD), and Misclassification rate (MCR).

We repeat stratified cross validation 10 times, average results and calculate standard deviations in Table \ref{table:CV}.
\begin{table}[H]
    \label{table:CV}
\scriptsize
\center
\begin{sc}
\begin{tabular}{ll|c|c}
\hline
\multicolumn{2}{c|}{Method}                       & MAD & MCR\\
\hline
\multirow{2}{*}{Our method} & $\hat y^{(\text{Median})}_\omega$ & \begin{tabular}[c]{@{}c@{}} 0.1607 (0.00046)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.1606 (0.00047)
\end{tabular} \\
&$\hat y^{(\text{Mode})}_\omega$         & \begin{tabular}[c]{@{}c@{}}0.1607  (0.00046)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.1606 (0.00047)\end{tabular} \\ \cline{1-2}
\multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}Cotinuous method\\~\cite{filipovic2015tucker}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.2530  (0.00015)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.1599 (0.00023)\end{tabular} \\ \cline{1-2}
\multicolumn{2}{l|}{\begin{tabular}[c]{@{}l@{}}1bit Tensor Completion\\~\cite{ghadermarzy2018learning}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.3566 (0.00102)\end{tabular} & \begin{tabular}[c]{@{}c@{}}0.3563 (0.00102)\end{tabular} \\ \hline
\end{tabular}
\end{sc}
\caption{Comparison of tensor completion performance on HCP data: values in the parentheses are standard deviation from 10 repetitions.}
\end{table}



\subsection{InCarMusic recommendation system}
InCarMusic is a mobile application that offers music recommendation to passengers of cars based on contexts (Baltrunas et al. 2011). To be specific, the music recommendations can be different according to the mood of the driver or the traffic condition in this system. Those kinds of external events that have influence on ratings on the musics are regarded as the contexts. Our goal is to perform the tensor completion to the $42\times 139\times 26$ ordinal tensor and thereby we can offer context-specific music recommendation to users. The tensor entries consist of ordinal observations on the scale 1 to 5, the ratings of 42 users to 139 songs on 26 contexts and are encoded as NA for missing values. The number of missing values is 148,904 and the number of available values is 2,884.
We perform 5 fold cross validation to assess the performance our model on InCarMusic data. The estimation methods and the procedure are the same as in HCP data analysis.
Results are averaged from 5 fold cross validation. {\color{red} add s.e. to the above table.}

\begin{table}[H]
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|ccc|c}
\hline
\multirow{2}{*}{Critera}&\multicolumn{3}{c|}{Our method}&Continuous method\\
& $\hat y^{(\text{Mean})}_\omega$ & $\hat y^{(\text{Median})}_\omega$ & $\hat y^{(\text{Mode})}_\omega$ &~\cite{filipovic2015tucker}\\
\hline
MSE& \color{red}2.01&2.14 & 3.64&8.18\\
MAD& 1.23  &  {\color{red}1.13}&1.26&2.36\\
MCR& 0.80&0.75&{\color{red}0.54}&0.96\\
\hline
\end{tabular}
}
\caption{Comparison of tensor completion performance on InCarMusic dataset.}
\end{table}

We also attempted to run matrix ordinal mehod~\citep{bhaskar2016probabilistic} to the InCarMusic dataset. Unfortunately, the $\mb$ cannot be estimated from their method.

\bibliography{ICML.bib}
\bibliographystyle{icml2020}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2020. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
