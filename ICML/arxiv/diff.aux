\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baltrunas2011incarmusic}
\citation{nickel2011three}
\citation{hore2016tensor}
\citation{zhou2013tensor}
\citation{xia2019sup,zeng2019multiway}
\citation{hong2018generalized,zeng2019multiway}
\citation{ghadermarzy2018learning,ghadermarzy2019near}
\citation{kolda2009tensor,acar2010scalable}
\citation{acar2010scalable,anandkumar2014tensor}
\citation{hitchcock1927expression}
\citation{tucker1966some}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kolda2009tensor,ghadermarzy2019near}
\citation{kolda2009tensor,ghadermarzy2019near}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\citation{cai2013max,davenport2014,bhaskar20151}
\citation{cai2013max,davenport2014,bhaskar20151}
\citation{bhaskar2016probabilistic}
\citation{bhaskar2016probabilistic}
\citation{wang2018learning,hong2018generalized,ghadermarzy2018learning}
\citation{mccullagh1980regression}
\citation{mccullagh1980regression}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\color {blue}\uwave  {Comparison with previous work when tensor rank $r=O(1)$ (neglecting log factors). For ease of presentation, we summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode. }}}}{2}{table.1}}
\newlabel{tab:compare}{{1}{2}{\DIFaddFL {Comparison with previous work when tensor rank $r=O(1)$ (neglecting log factors). For ease of presentation, we summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode. }}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{3}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model formulation and motivation}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation model}{3}{subsection.3.1}}
\newlabel{eq:model}{{1}{3}{Observation model}{equation.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{eq:logodd}{{2}{3}{Observation model}{equation.3.2}{}}
\citation{mccullagh1980regression}
\citation{mccullagh1980regression}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Latent-variable interpretation}{4}{subsection.3.2}}
\newlabel{sec:latent}{{3.2}{4}{Latent-variable interpretation}{subsection.3.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:quantization}{{3}{4}{Latent-variable interpretation}{equation.3.3}{}}
\newlabel{eq:latent}{{4}{4}{Latent-variable interpretation}{equation.3.4}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\newlabel{ass:link}{{1}{4}{}{assumption.1}{}}
\citation{zhou2013tensor,bhaskar20151}
\citation{zhou2013tensor,bhaskar20151}
\citation{kolda2009tensor}
\citation{kolda2009tensor}
\citation{hitchcock1927expression}
\citation{hitchcock1927expression}
\citation{oseledets2011tensor}
\citation{oseledets2011tensor}
\citation{de2008tensor}
\citation{de2008tensor}
\citation{negahban2011estimation,cai2013max,bhaskar20151}
\citation{negahban2011estimation,cai2013max,bhaskar20151}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Problem 1: Tensor denoising}{5}{subsection.3.3}}
\newlabel{sec:denoising}{{3.3}{5}{Problem 1: Tensor denoising}{subsection.3.3}{}}
\newlabel{eq:space}{{5}{5}{Problem 1: Tensor denoising}{equation.3.5}{}}
\newlabel{eq:Tucker}{{6}{5}{Problem 1: Tensor denoising}{equation.3.6}{}}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Problem 2: Tensor completion}{5}{subsection.3.4}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:space}
\@writefile{toc}{\contentsline {section}{\numberline {4}Rank-constrained M-estimator}{6}{section.4}}
\newlabel{sec:theory}{{4}{6}{Rank-constrained M-estimator}{section.4}{}}
\newlabel{eq:objective}{{7}{6}{Rank-constrained M-estimator}{equation.4.7}{}}
\newlabel{eq:estimator}{{8}{6}{Rank-constrained M-estimator}{equation.4.8}{}}
\newlabel{eq:regular}{{4}{6}{Rank-constrained M-estimator}{equation.4.8}{}}
\MT@newlabel{eq:latent}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Estimation error for tensor denoising}{7}{subsection.4.1}}
\newlabel{sec:denosing}{{4.1}{7}{Estimation error for tensor denoising}{subsection.4.1}{}}
\MT@newlabel{eq:estimator}
\newlabel{thm:rate}{{4.1}{7}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:estimator}
\newlabel{eq:rate}{{9}{7}{Statistical convergence}{equation.4.9}{}}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:estimator}
\newlabel{cor:prediction}{{4.1}{7}{Prediction error}{cor.4.1}{}}
\newlabel{eq:KLrate}{{10}{7}{Prediction error}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\newlabel{eq:ours}{{4.1}{7}{Estimation error for tensor denoising}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:KLrate}
\MT@newlabel{eq:estimator}
\newlabel{thm:minimax}{{4.2}{8}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:lower}{{4.2}{8}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:estimator}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sample complexity for tensor completion}{8}{subsection.4.2}}
\newlabel{eq:weighted}{{11}{8}{Sample complexity for tensor completion}{equation.4.11}{}}
\MT@newlabel{eq:weighted}
\citation{mu2014square}
\citation{mu2014square}
\citation{yuan2016tensor}
\citation{yuan2016tensor}
\citation{zhang2019cross}
\citation{zhang2019cross}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\newlabel{thm:completion}{{4.3}{9}{}{thm.4.3}{}}
\MT@newlabel{eq:estimator}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Implementation}{9}{section.5}}
\newlabel{sec:algorithm}{{5}{9}{Numerical Implementation}{section.5}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:Tucker}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:model}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Ordinal tensor decomposition}}{10}{algorithm.1}}
\newlabel{alg}{{1}{10}{Numerical Implementation}{algorithm.1}{}}
\newlabel{fig:stability}{{5}{10}{Numerical Implementation}{ALC@unique.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Trajectory of objective function with various $d$ and $r$.}}{10}{figure.1}}
\newlabel{eq:BIC}{{5}{10}{Numerical Implementation}{figure.1}{}}
\citation{davenport2014,sur2019modern}
\citation{davenport2014,sur2019modern}
\citation{acar2010scalable}
\citation{acar2010scalable}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{11}{section.6}}
\newlabel{sec:experiment}{{6}{11}{Experiments}{section.6}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:Tucker}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Finite-sample performance}{11}{subsection.6.1}}
\newlabel{sec:simulation}{{6.1}{11}{Finite-sample performance}{subsection.6.1}{}}
\MT@newlabel{eq:rate}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with alternative methods}{11}{subsection.6.2}}
\newlabel{sec:compare}{{6.2}{11}{Comparison with alternative methods}{subsection.6.2}{}}
\citation{bhaskar2016probabilistic}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\delimiter 69645069 \mathaccentV {hat}05E\Theta -\Theta ^{\text  {true}}\delimiter 86422285 _F/\delimiter 69645069 \Theta ^{\text  {true}}\delimiter 86422285 _F$ for better visualization.}}{12}{figure.2}}
\newlabel{fig:finite}{{2}{12}{Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\FnormSize {}{\hat \Theta -\trueT }/\FnormSize {}{\trueT }$ for better visualization}{figure.2}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance comparison {\color {red}\sout  {in MCR }}{\color {blue}\uwave  {for predicting most likely labels. }}(a, {\color {red}\sout  {b) and MAD (}}c{\color {red}\sout  {, d}}) {\color {blue}\uwave  {Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$}}. (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. {\color {red}\sout  {(a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. }}}}{13}{figure.3}}
\newlabel{fig:compare}{{3}{13}{Performance comparison \DIFdelbeginFL \DIFdelFL {in MCR }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {for predicting most likely labels. }\DIFaddendFL (a, \DIFdelbeginFL \DIFdelFL {b) and MAD (}\DIFdelendFL c\DIFdelbeginFL \DIFdelFL {, d}\DIFdelendFL ) \DIFaddbeginFL \DIFaddFL {Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$}\DIFaddendFL . (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. \DIFdelbeginFL \DIFdelFL {(a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. }\DIFdelendFL }{figure.3}{}}
\MT@newlabel{eq:latent}
\citation{geddes2016human}
\citation{van2013wu}
\citation{baltrunas2011incarmusic}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\color {blue}\uwave  {Performance comparison for predicting median labels. (a,c) Prediction errors versus sample complexity $\rho =|\Omega |/d^k$ when $L=5$. (b,d) Prediction errors versus the number of ordinal levels $L$, when $\rho =0.8.$}}}}{14}{figure.4}}
\newlabel{fig:compare2}{{4}{14}{\DIFaddFL {Performance comparison for predicting median labels. (a,c) Prediction errors versus sample complexity $\rho =|\Omega |/d^k$ when $L=5$. (b,d) Prediction errors versus the number of ordinal levels $L$, when $\rho =0.8.$}}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Applications}{14}{section.7}}
\citation{stoeckel2009supramarginal}
\citation{baltrunas2011incarmusic}
\citation{baltrunas2011incarmusic}
\citation{chen2019non}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}{\color {red}\sout  {Human Connectome Project (HCP)}}}{15}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}{\color {red}\sout  {InCarMusic recommendation system}}}{15}{subsection.7.1}}
\@writefile{toc}{\contentsline {section}{\numberline {8}{\color {red}\sout  {Conclusions}}{\color {blue}\uwave  {Proofs}}}{15}{section.8}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces {\color {blue}\uwave  {Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses.}}}}{16}{table.2}}
\newlabel{table:CV}{{2}{16}{\DIFaddFL {Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses.}}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}{\color {red}\sout  {Proofs}}}{16}{section.9}}
\newlabel{sec:proof}{{9}{16}{\DIFdel {Proofs}}{section.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Estimation error for tensor denoising}{16}{subsection.8.1}}
\newlabel{sec:proofMSE}{{8.1}{16}{Estimation error for tensor denoising}{subsection.8.1}{}}
\newlabel{eq:property}{{12}{16}{Estimation error for tensor denoising}{equation.8.12}{}}
\MT@newlabel{eq:property}
\newlabel{eq:bound}{{13}{16}{Estimation error for tensor denoising}{equation.8.13}{}}
\newlabel{eq:taylor}{{14}{16}{Estimation error for tensor denoising}{equation.8.14}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:linear}{{15}{17}{Estimation error for tensor denoising}{equation.8.15}{}}
\MT@newlabel{eq:property}
\newlabel{eq:norm}{{16}{17}{Estimation error for tensor denoising}{equation.8.16}{}}
\newlabel{eq:normrandom}{{17}{17}{Estimation error for tensor denoising}{equation.8.17}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:norm}
\MT@newlabel{eq:normrandom}
\newlabel{eq:linearconclusion}{{18}{17}{Estimation error for tensor denoising}{equation.8.18}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:quadratic}{{19}{17}{Estimation error for tensor denoising}{equation.8.19}{}}
\MT@newlabel{eq:taylor}
\MT@newlabel{eq:linearconclusion}
\MT@newlabel{eq:quadratic}
\newlabel{eq:KLbound1}{{20}{18}{Estimation error for tensor denoising}{equation.8.20}{}}
\MT@newlabel{eq:KLbound1}
\newlabel{eq:KLbound}{{21}{18}{Estimation error for tensor denoising}{equation.8.21}{}}
\MT@newlabel{eq:KLbound}
\newlabel{eq:totalKL}{{22}{18}{Estimation error for tensor denoising}{equation.8.22}{}}
\MT@newlabel{eq:totalKL}
\newlabel{eq:final}{{23}{18}{Estimation error for tensor denoising}{equation.8.23}{}}
\MT@newlabel{eq:final}
\newlabel{eq:prob}{{8.1}{18}{Estimation error for tensor denoising}{equation.8.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Sample complexity for tensor completion}{19}{subsection.8.2}}
\newlabel{eq:Taylor2}{{24}{19}{Sample complexity for tensor completion}{equation.8.24}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:quadratic}
\newlabel{eq:linear2}{{8.2}{19}{Sample complexity for tensor completion}{Item.4}{}}
\newlabel{eq:quadratic2}{{25}{19}{Sample complexity for tensor completion}{equation.8.25}{}}
\MT@newlabel{eq:Taylor2}
\MT@newlabel{eq:quadratic2}
\newlabel{eq:sample}{{26}{19}{Sample complexity for tensor completion}{equation.8.26}{}}
\MT@newlabel{eq:sample}
\citation{brascamp2002extensions}
\citation{ghadermarzy2019near}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Convexity of the log-likelihood function}{20}{subsection.8.3}}
\newlabel{sec:proofconvexity}{{8.3}{20}{Convexity of the log-likelihood function}{subsection.8.3}{}}
\newlabel{thm:convexity}{{8.1}{20}{}{thm.8.1}{}}
\newlabel{eq:function}{{27}{20}{}{equation.8.27}{}}
\MT@newlabel{eq:ass}
\MT@newlabel{eq:function}
\newlabel{lem:lossconvexity}{{1}{20}{Corollary 3.5 in~\cite {brascamp2002extensions}}{lem.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Auxiliary lemmas}{20}{subsection.8.4}}
\newlabel{sec:lemma}{{8.4}{20}{Auxiliary lemmas}{subsection.8.4}{}}
\citation{lim2005singular}
\citation{friedland2018nuclear}
\citation{ghadermarzy2019near}
\citation{wang2017operator}
\citation{jiang2017tensor}
\citation{friedland2018nuclear}
\newlabel{lem:Mnormbound}{{2}{21}{M-norm and infinity norm~\citep {ghadermarzy2019near}}{lem.2}{}}
\newlabel{lem:nuclear}{{3}{21}{Nuclear norm and F-norm}{lem.3}{}}
\newlabel{eq:norminequality}{{28}{21}{Auxiliary lemmas}{equation.8.28}{}}
\MT@newlabel{eq:norminequality}
\newlabel{lem:inq}{{4}{21}{}{lem.4}{}}
\citation{nguyen2015tensor}
\citation{tomioka2014spectral}
\citation{tomioka2014spectral}
\newlabel{lem:tensor}{{5}{22}{\cite {tomioka2014spectral}}{lem.5}{}}
\newlabel{lem:noisytensor}{{6}{22}{}{lem.6}{}}
\newlabel{lem:KLentry}{{7}{23}{}{lem.7}{}}
\newlabel{eq:KL}{{8.4}{23}{Auxiliary lemmas}{lem.7}{}}
\newlabel{lem:KL}{{8}{23}{KL divergence and F-norm}{lem.8}{}}
\MT@newlabel{eq:model}
\newlabel{eq:ass}{{29}{23}{KL divergence and F-norm}{equation.8.29}{}}
\MT@newlabel{eq:model}
\newlabel{eq:entrywise}{{30}{24}{Auxiliary lemmas}{equation.8.30}{}}
\MT@newlabel{eq:ass}
\MT@newlabel{eq:entrywise}
\newlabel{lem:construction}{{9}{24}{}{lem.9}{}}
\citation{tsybakov2008introduction}
\citation{ghadermarzy2019near}
\newlabel{lem:VGbound}{{10}{25}{Varshamov-Gilbert bound}{lem.10}{}}
\newlabel{lem:Tsybakov}{{11}{25}{Theorem 2.5 in~\cite {tsybakov2008introduction}}{lem.11}{}}
\newlabel{lem:convexity}{{12}{25}{Lemma 28 in~\cite {ghadermarzy2019near}}{lem.12}{}}
\citation{ge2017optimization,chen2019non}
\@writefile{toc}{\contentsline {section}{\numberline {9}{\color {blue}\uwave  {Conclusions}}}{26}{section.9}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Extension of Theorem\nobreakspace  {}\ref  {thm:rate} to unknown cut-off points}{27}{appendix.A}}
\newlabel{sec:extention}{{A}{27}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{appendix.A}{}}
\newlabel{eq:joint}{{31}{27}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.31}{}}
\MT@newlabel{eq:bound}
\newlabel{eq:UL}{{32}{27}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.32}{}}
\newlabel{eq:CD}{{33}{27}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.33}{}}
\newlabel{ass:joint}{{2}{27}{}{assumption.2}{}}
\newlabel{thm:ratejoint}{{A.1}{28}{Statistical convergence with unknown $\mb $}{thm.A.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:joint}
\MT@newlabel{eq:UL}
\MT@newlabel{eq:CD}
\newlabel{eq:level}{{34}{28}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.34}{}}
\MT@newlabel{eq:level}
\newlabel{eq:ratenew}{{A}{29}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.34}{}}
\newlabel{eq:propertyb}{{A}{29}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.34}{}}
\newlabel{eq:taylorb}{{35}{29}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.35}{}}
\MT@newlabel{eq:taylorb}
\newlabel{eq:linearb}{{36}{29}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.36}{}}
\MT@newlabel{eq:taylorb}
\newlabel{eq:quadraticb}{{37}{29}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.37}{}}
\MT@newlabel{eq:taylorb}
\MT@newlabel{eq:linearb}
\MT@newlabel{eq:quadraticb}
\@writefile{toc}{\contentsline {section}{\numberline {B}{\color {red}\sout  {Additional results on simulations}}}{30}{appendix.B}}
\citation{kolda2009tensor}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional results on HCP analysis}{31}{appendix.B}}
\newlabel{sec:additionalHCP}{{B}{31}{Additional results on HCP analysis}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Clustering based on Tucker representation}{31}{subsection.B.1}}
\newlabel{sec:clustering}{{B.1}{31}{Clustering based on Tucker representation}{subsection.B.1}{}}
\newlabel{eq:Tuckerest}{{38}{31}{Clustering based on Tucker representation}{equation.B.38}{}}
\MT@newlabel{eq:Tuckerest}
\citation{carlson2012physiology,reed1994nature}
\citation{stoeckel2009supramarginal}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Clustering results on HCP}{32}{subsection.B.2}}
\@writefile{lot}{\contentsline {table}{\numberline {S1}{\ignorespaces Clustering result of brain nodes. The first alphabet in the node name indicates the left {\color {red}\sout  {brain }}(L) {\color {red}\sout  {and the }}{\color {blue}\uwave  {or }}right {\color {red}\sout  {brain }}(R) {\color {red}\sout  {and the values }}{\color {blue}\uwave  {hemisphere. The number }}in the parentheses {\color {red}\sout  {are }}{\color {blue}\uwave  {indicates }}the {\color {red}\sout  {number of nodes that have the same name }}{\color {blue}\uwave  {node count }}in each cluster. }}{32}{table.1}}
\newlabel{table:clustering}{{S1}{32}{Clustering result of brain nodes. The first alphabet in the node name indicates the left \DIFdelbeginFL \DIFdelFL {brain }\DIFdelendFL (L) \DIFdelbeginFL \DIFdelFL {and the }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {or }\DIFaddendFL right \DIFdelbeginFL \DIFdelFL {brain }\DIFdelendFL (R) \DIFdelbeginFL \DIFdelFL {and the values }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {hemisphere. The number }\DIFaddendFL in the parentheses \DIFdelbeginFL \DIFdelFL {are }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {indicates }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL {number of nodes that have the same name }\DIFdelendFL \DIFaddbeginFL \DIFaddFL {node count }\DIFaddendFL in each cluster}{table.1}{}}
\bibdata{tensor_wang}
\bibcite{acar2010scalable}{{1}{2010}{{Acar et~al.}}{{}}}
\bibcite{anandkumar2014tensor}{{2}{2014}{{Anandkumar et~al.}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces {\color {blue}\uwave  {Top two clusters of brain nodes. (a) Nodes in yellow are from cluster I (left hemisphere), and nodes in green are from cluster II (right hemisphere). (b) Predicted connectivity within cluster I and cluster II. Edges are colored based on predicted strength level averaged across individuals.}}}}{33}{figure.1}}
\newlabel{figure:brain image}{{S1}{33}{\DIFaddFL {Top two clusters of brain nodes. (a) Nodes in yellow are from cluster I (left hemisphere), and nodes in green are from cluster II (right hemisphere). (b) Predicted connectivity within cluster I and cluster II. Edges are colored based on predicted strength level averaged across individuals.}}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces {\color {blue}\uwave  {Elbow plot for determining the number of clusters in $K$-means.}}}}{33}{figure.2}}
\newlabel{figure:elbow}{{S2}{33}{\DIFaddFL {Elbow plot for determining the number of clusters in $K$-means.}}{figure.2}{}}
\bibcite{baltrunas2011incarmusic}{{3}{2011}{{Baltrunas et~al.}}{{}}}
\bibcite{bhaskar2016probabilistic}{{4}{2016}{{Bhaskar}}{{}}}
\bibcite{bhaskar20151}{{5}{2015}{{Bhaskar and Javanmard}}{{}}}
\bibcite{brascamp2002extensions}{{6}{2002}{{Brascamp and Lieb}}{{}}}
\bibcite{cai2013max}{{7}{2013}{{Cai and Zhou}}{{}}}
\bibcite{chen2019non}{{8}{2019}{{Chen et~al.}}{{}}}
\bibcite{davenport2014}{{9}{2014}{{Davenport et~al.}}{{}}}
\bibcite{de2008tensor}{{10}{2008}{{De~Silva and Lim}}{{}}}
\bibcite{friedland2018nuclear}{{11}{2018}{{Friedland and Lim}}{{}}}
\bibcite{ge2017optimization}{{12}{2017}{{Ge and Ma}}{{}}}
\bibcite{ghadermarzy2018learning}{{13}{2018}{{Ghadermarzy et~al.}}{{}}}
\bibcite{ghadermarzy2019near}{{14}{2019}{{Ghadermarzy et~al.}}{{}}}
\bibcite{hitchcock1927expression}{{15}{1927}{{Hitchcock}}{{}}}
\bibcite{hong2018generalized}{{16}{2019}{{Hong et~al.}}{{}}}
\bibcite{hore2016tensor}{{17}{2016}{{Hore et~al.}}{{}}}
\bibcite{jiang2017tensor}{{18}{2017}{{Jiang et~al.}}{{}}}
\bibcite{kolda2009tensor}{{19}{2009}{{Kolda and Bader}}{{}}}
\bibcite{lim2005singular}{{20}{2005}{{Lim}}{{}}}
\bibcite{mccullagh1980regression}{{21}{1980}{{McCullagh}}{{}}}
\bibcite{mu2014square}{{22}{2014}{{Mu et~al.}}{{}}}
\bibcite{negahban2011estimation}{{23}{2011}{{Negahban et~al.}}{{}}}
\bibcite{nguyen2015tensor}{{24}{2015}{{Nguyen et~al.}}{{}}}
\bibcite{nickel2011three}{{25}{2011}{{Nickel et~al.}}{{}}}
\bibcite{oseledets2011tensor}{{26}{2011}{{Oseledets}}{{}}}
\bibcite{reed1994nature}{{27}{1994}{{Reed and Caselli}}{{}}}
\bibcite{stoeckel2009supramarginal}{{28}{2009}{{Stoeckel et~al.}}{{}}}
\bibcite{sur2019modern}{{29}{2019}{{Sur and Cand{\`e}s}}{{}}}
\bibcite{tomioka2014spectral}{{30}{2014}{{Tomioka and Suzuki}}{{}}}
\bibcite{tsybakov2008introduction}{{31}{2008}{{Tsybakov}}{{}}}
\bibcite{tucker1966some}{{32}{1966}{{Tucker}}{{}}}
\bibcite{van2013wu}{{33}{2013}{{Van~Essen et~al.}}{{}}}
\bibcite{wang2017operator}{{34}{2017}{{Wang et~al.}}{{}}}
\bibcite{wang2018learning}{{35}{2018}{{Wang and Li}}{{}}}
\bibcite{zeng2019multiway}{{36}{2019}{{Wang and Zeng}}{{}}}
\bibcite{xia2019sup}{{37}{2019}{{Xia and Zhou}}{{}}}
\bibcite{yuan2016tensor}{{38}{2016}{{Yuan and Zhang}}{{}}}
\bibcite{zhang2019cross}{{39}{2019}{{Zhang et~al.}}{{}}}
\bibcite{zhou2013tensor}{{40}{2013}{{Zhou et~al.}}{{}}}
\bibstyle{apalike}
