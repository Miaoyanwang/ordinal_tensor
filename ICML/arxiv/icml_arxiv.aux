\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{baltrunas2011incarmusic}
\citation{nickel2011three}
\citation{hore2016tensor}
\citation{zhou2013tensor}
\citation{xia2019sup,zeng2019multiway}
\citation{ghadermarzy2018learning,ghadermarzy2019near}
\citation{kolda2009tensor,acar2010scalable}
\citation{hitchcock1927expression}
\citation{tucker1966some}
\citation{kolda2009tensor,ghadermarzy2019near}
\citation{ghadermarzy2018learning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ghadermarzy2018learning}
\citation{cai2013max,davenport2014,bhaskar20151}
\citation{bhaskar2016probabilistic}
\citation{wang2018learning,hong2018generalized,ghadermarzy2018learning}
\citation{mccullagh1980regression}
\citation{ghadermarzy2018learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison with previous work. For ease of presentation, we summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode. }}{2}{table.1}}
\newlabel{tab:compare}{{1}{2}{Comparison with previous work. For ease of presentation, we summarize the error rate and sample complexity assuming equal tensor dimension in all modes. $K$: tensor order; $L$: number of ordinal levels; $d$: dimension at each mode}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{3}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model formulation and motivation}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation model}{3}{subsection.3.1}}
\newlabel{eq:model}{{1}{3}{Observation model}{equation.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{eq:logodd}{{2}{3}{Observation model}{equation.3.2}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:logodd}
\citation{mccullagh1980regression}
\citation{zhou2013tensor,bhaskar20151}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Latent-variable interpretation}{4}{subsection.3.2}}
\newlabel{sec:latent}{{3.2}{4}{Latent-variable interpretation}{subsection.3.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:quantization}{{3}{4}{Latent-variable interpretation}{equation.3.3}{}}
\newlabel{eq:latent}{{4}{4}{Latent-variable interpretation}{equation.3.4}{}}
\MT@newlabel{eq:latent}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\MT@newlabel{eq:quantization}
\MT@newlabel{eq:model}
\newlabel{ass:link}{{1}{4}{}{assumption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Problem 1: Tensor denoising}{4}{subsection.3.3}}
\newlabel{sec:denoising}{{3.3}{4}{Problem 1: Tensor denoising}{subsection.3.3}{}}
\newlabel{eq:space}{{5}{4}{Problem 1: Tensor denoising}{equation.3.5}{}}
\citation{kolda2009tensor}
\citation{hitchcock1927expression}
\citation{oseledets2011tensor}
\citation{de2008tensor}
\citation{negahban2011estimation,cai2013max,bhaskar20151}
\newlabel{eq:Tucker}{{6}{5}{Problem 1: Tensor denoising}{equation.3.6}{}}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Problem 2: Tensor completion}{5}{subsection.3.4}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:space}
\@writefile{toc}{\contentsline {section}{\numberline {4}Rank-constrained M-estimator}{5}{section.4}}
\newlabel{sec:theory}{{4}{5}{Rank-constrained M-estimator}{section.4}{}}
\newlabel{eq:objective}{{7}{5}{Rank-constrained M-estimator}{equation.4.7}{}}
\newlabel{eq:estimator}{{8}{6}{Rank-constrained M-estimator}{equation.4.8}{}}
\newlabel{eq:regular}{{4}{6}{Rank-constrained M-estimator}{equation.4.8}{}}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Estimation error for tensor denoising}{6}{subsection.4.1}}
\newlabel{sec:denosing}{{4.1}{6}{Estimation error for tensor denoising}{subsection.4.1}{}}
\MT@newlabel{eq:estimator}
\newlabel{thm:rate}{{4.1}{6}{Statistical convergence}{thm.4.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:estimator}
\newlabel{eq:rate}{{9}{6}{Statistical convergence}{equation.4.9}{}}
\MT@newlabel{eq:estimator}
\MT@newlabel{eq:estimator}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\newlabel{cor:prediction}{{1}{7}{Prediction error}{cor.1}{}}
\newlabel{eq:KLrate}{{10}{7}{Prediction error}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\newlabel{eq:ours}{{4.1}{7}{Estimation error for tensor denoising}{equation.4.10}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:KLrate}
\MT@newlabel{eq:estimator}
\newlabel{thm:minimax}{{4.2}{7}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:lower}{{4.2}{7}{Minimax lower bound}{thm.4.2}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:estimator}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sample complexity for tensor completion}{7}{subsection.4.2}}
\citation{mu2014square}
\citation{yuan2016tensor}
\citation{zhang2019cross}
\citation{ghadermarzy2018learning}
\newlabel{eq:weighted}{{11}{8}{Sample complexity for tensor completion}{equation.4.11}{}}
\MT@newlabel{eq:weighted}
\newlabel{thm:completion}{{4.3}{8}{}{thm.4.3}{}}
\MT@newlabel{eq:estimator}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Implementation}{8}{section.5}}
\newlabel{sec:algorithm}{{5}{8}{Numerical Implementation}{section.5}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:Tucker}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:estimator}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Ordinal tensor decomposition}}{9}{algorithm.1}}
\newlabel{alg}{{1}{9}{Numerical Implementation}{algorithm.1}{}}
\MT@newlabel{eq:model}
\newlabel{fig:stability}{{5}{9}{Numerical Implementation}{ALC@unique.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Trajectory of objective function with various $d$ and $r$.}}{9}{figure.1}}
\citation{davenport2014,sur2019modern}
\newlabel{eq:BIC}{{5}{10}{Numerical Implementation}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{10}{section.6}}
\newlabel{sec:experiment}{{6}{10}{Experiments}{section.6}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:Tucker}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Finite-sample performance}{10}{subsection.6.1}}
\newlabel{sec:simulation}{{6.1}{10}{Finite-sample performance}{subsection.6.1}{}}
\MT@newlabel{eq:rate}
\citation{acar2010scalable}
\citation{ghadermarzy2018learning}
\citation{bhaskar2016probabilistic}
\citation{ghadermarzy2018learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\delimiter 69645069 \mathaccentV {hat}05E\Theta -\Theta ^{\text  {true}}\delimiter 86422285 _F/\delimiter 69645069 \Theta ^{\text  {true}}\delimiter 86422285 _F$ for better visualization.}}{11}{figure.2}}
\newlabel{fig:finite}{{2}{11}{Empirical relationship between (relative) MSE versus (a) dimension $d$, (b) signal level $\alpha $, (c) observation fraction $\rho $, and (d) number of ordinal levels $L$. In panels (b)-(d), we plot the relative MSE $=\FnormSize {}{\hat \Theta -\trueT }/\FnormSize {}{\trueT }$ for better visualization}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with alternative methods}{11}{subsection.6.2}}
\newlabel{sec:compare}{{6.2}{11}{Comparison with alternative methods}{subsection.6.2}{}}
\MT@newlabel{eq:model}
\citation{ghadermarzy2018learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance comparison in MCR (a, b) and MAD (c, d). (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. (a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$. }}{12}{figure.3}}
\newlabel{fig:compare}{{3}{12}{Performance comparison in MCR (a, b) and MAD (c, d). (b, d) Prediction errors versus the number of ordinal levels $L$ when $\rho =0.8$. (a, c) Prediction errors versus sample complexity $\rho =|\Omega |/d^K$ when $L=5$}{figure.3}{}}
\MT@newlabel{eq:logodd}
\citation{geddes2016human}
\citation{baltrunas2011incarmusic}
\citation{baltrunas2011incarmusic}
\citation{chen2019non}
\MT@newlabel{eq:latent}
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Applications}{13}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Human Connectome Project (HCP)}{13}{subsection.7.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses.}}{13}{table.2}}
\newlabel{table:CV}{{2}{13}{Comparison of prediction error in the HPC and InCarMusic analyses. Standard errors are reported in parentheses}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}InCarMusic recommendation system}{13}{subsection.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions}{14}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Proofs}{14}{section.9}}
\newlabel{sec:proof}{{9}{14}{Proofs}{section.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Estimation error for tensor denoising}{14}{subsection.9.1}}
\newlabel{eq:property}{{12}{14}{Estimation error for tensor denoising}{equation.9.12}{}}
\MT@newlabel{eq:property}
\newlabel{eq:bound}{{13}{14}{Estimation error for tensor denoising}{equation.9.13}{}}
\newlabel{eq:taylor}{{14}{14}{Estimation error for tensor denoising}{equation.9.14}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:linear}{{15}{14}{Estimation error for tensor denoising}{equation.9.15}{}}
\MT@newlabel{eq:property}
\newlabel{eq:norm}{{16}{15}{Estimation error for tensor denoising}{equation.9.16}{}}
\newlabel{eq:normrandom}{{17}{15}{Estimation error for tensor denoising}{equation.9.17}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:norm}
\MT@newlabel{eq:normrandom}
\newlabel{eq:linearconclusion}{{18}{15}{Estimation error for tensor denoising}{equation.9.18}{}}
\MT@newlabel{eq:taylor}
\newlabel{eq:quadratic}{{19}{15}{Estimation error for tensor denoising}{equation.9.19}{}}
\MT@newlabel{eq:taylor}
\MT@newlabel{eq:linearconclusion}
\MT@newlabel{eq:quadratic}
\newlabel{eq:KLbound1}{{20}{16}{Estimation error for tensor denoising}{equation.9.20}{}}
\MT@newlabel{eq:KLbound1}
\newlabel{eq:KLbound}{{21}{16}{Estimation error for tensor denoising}{equation.9.21}{}}
\MT@newlabel{eq:KLbound}
\newlabel{eq:totalKL}{{22}{16}{Estimation error for tensor denoising}{equation.9.22}{}}
\MT@newlabel{eq:totalKL}
\newlabel{eq:final}{{23}{16}{Estimation error for tensor denoising}{equation.9.23}{}}
\MT@newlabel{eq:final}
\newlabel{eq:prob}{{9.1}{16}{Estimation error for tensor denoising}{equation.9.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Sample complexity for tensor completion}{16}{subsection.9.2}}
\newlabel{eq:Taylor2}{{24}{16}{Sample complexity for tensor completion}{equation.9.24}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:quadratic}
\newlabel{eq:linear2}{{9.2}{17}{Sample complexity for tensor completion}{Item.4}{}}
\newlabel{eq:quadratic2}{{25}{17}{Sample complexity for tensor completion}{equation.9.25}{}}
\MT@newlabel{eq:Taylor2}
\MT@newlabel{eq:quadratic2}
\newlabel{eq:sample}{{26}{17}{Sample complexity for tensor completion}{equation.9.26}{}}
\MT@newlabel{eq:sample}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Convexity of the log-likelihood function}{17}{subsection.9.3}}
\newlabel{thm:convexity}{{9.1}{17}{}{thm.9.1}{}}
\newlabel{eq:function}{{27}{17}{}{equation.9.27}{}}
\MT@newlabel{eq:ass}
\citation{brascamp2002extensions}
\citation{ghadermarzy2019near}
\citation{lim2005singular}
\citation{friedland2018nuclear}
\citation{ghadermarzy2019near}
\MT@newlabel{eq:function}
\newlabel{lem:lossconvexity}{{1}{18}{Corollary 3.5 in~\cite {brascamp2002extensions}}{lem.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Auxiliary lemmas}{18}{subsection.9.4}}
\newlabel{sec:lemma}{{9.4}{18}{Auxiliary lemmas}{subsection.9.4}{}}
\citation{wang2017operator}
\citation{jiang2017tensor}
\citation{friedland2018nuclear}
\citation{nguyen2015tensor}
\citation{tomioka2014spectral}
\citation{tomioka2014spectral}
\newlabel{lem:Mnormbound}{{2}{19}{M-norm and infinity norm~\citep {ghadermarzy2019near}}{lem.2}{}}
\newlabel{lem:nuclear}{{3}{19}{Nuclear norm and F-norm}{lem.3}{}}
\newlabel{eq:norminequality}{{28}{19}{Auxiliary lemmas}{equation.9.28}{}}
\MT@newlabel{eq:norminequality}
\newlabel{lem:inq}{{4}{19}{}{lem.4}{}}
\newlabel{lem:tensor}{{5}{19}{\cite {tomioka2014spectral}}{lem.5}{}}
\newlabel{lem:noisytensor}{{6}{20}{}{lem.6}{}}
\newlabel{lem:KLentry}{{7}{20}{}{lem.7}{}}
\newlabel{eq:KL}{{9.4}{20}{Auxiliary lemmas}{lem.7}{}}
\newlabel{lem:KL}{{8}{21}{KL divergence and F-norm}{lem.8}{}}
\MT@newlabel{eq:model}
\newlabel{eq:ass}{{29}{21}{KL divergence and F-norm}{equation.9.29}{}}
\MT@newlabel{eq:model}
\newlabel{eq:entrywise}{{30}{21}{Auxiliary lemmas}{equation.9.30}{}}
\MT@newlabel{eq:ass}
\MT@newlabel{eq:entrywise}
\newlabel{lem:construction}{{9}{21}{}{lem.9}{}}
\citation{tsybakov2009introduction}
\newlabel{lem:VGbound}{{10}{22}{Varshamov-Gilbert bound}{lem.10}{}}
\newlabel{lem:Tsybakov}{{11}{22}{Theorem 2.5 in~\cite {tsybakov2009introduction}}{lem.11}{}}
\citation{ghadermarzy2019near}
\newlabel{lem:convexity}{{12}{23}{Lemma 28 in~\cite {ghadermarzy2019near}}{lem.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Extension of Theorem\nobreakspace  {}\ref  {thm:rate} to unknown cut-off points}{23}{appendix.A}}
\newlabel{eq:joint}{{31}{23}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.31}{}}
\MT@newlabel{eq:bound}
\newlabel{eq:UL}{{32}{24}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.32}{}}
\newlabel{eq:CD}{{33}{24}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.33}{}}
\newlabel{ass:joint}{{2}{24}{}{assumption.2}{}}
\newlabel{thm:ratejoint}{{A.1}{24}{Statistical convergence with unknown $\mb $}{thm.A.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:joint}
\MT@newlabel{eq:UL}
\MT@newlabel{eq:CD}
\newlabel{eq:level}{{34}{25}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.34}{}}
\MT@newlabel{eq:level}
\newlabel{eq:ratenew}{{A}{25}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.34}{}}
\newlabel{eq:property}{{35}{25}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.35}{}}
\newlabel{eq:taylorb}{{36}{25}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.36}{}}
\MT@newlabel{eq:taylorb}
\newlabel{eq:linearb}{{37}{26}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.37}{}}
\MT@newlabel{eq:taylorb}
\newlabel{eq:quadraticb}{{38}{26}{Extension of Theorem~\ref {thm:rate} to unknown cut-off points}{equation.A.38}{}}
\MT@newlabel{eq:taylorb}
\MT@newlabel{eq:linearb}
\MT@newlabel{eq:quadraticb}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional results on simulations}{27}{appendix.B}}
\MT@newlabel{eq:latent}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance comparison in predicting the median label. (a,c) Prediction errors versus sample complexity $\rho =|\Omega |/d^k$ when $L=5$. (b,d) Prediction errors versus the number of ordinal levels $L$, when $\rho =0.8.$}}{27}{figure.4}}
\newlabel{fig:compare2}{{4}{27}{Performance comparison in predicting the median label. (a,c) Prediction errors versus sample complexity $\rho =|\Omega |/d^k$ when $L=5$. (b,d) Prediction errors versus the number of ordinal levels $L$, when $\rho =0.8.$}{figure.4}{}}
\citation{kolda2009tensor}
\@writefile{toc}{\contentsline {section}{\numberline {C}Additional results on HCP analysis}{28}{appendix.C}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Clustering based on Tucker representation}{28}{subsection.C.1}}
\newlabel{sec:clustering}{{C.1}{28}{Clustering based on Tucker representation}{subsection.C.1}{}}
\newlabel{eq:Tuckerest}{{39}{28}{Clustering based on Tucker representation}{equation.C.39}{}}
\MT@newlabel{eq:Tuckerest}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Clustering results on HCP}{28}{subsection.C.2}}
\citation{carlson2012physiology,reed1994nature}
\newlabel{figure:elbow}{{C.2}{29}{Clustering results on HCP}{subsection.C.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The elbow plot for the number of clusters}}{29}{figure.5}}
\newlabel{table:clustering}{{C.2}{29}{Clustering results on HCP}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Clustering result of brain nodes. The first alphabet in the node name indicates the left brain (L) and the right brain (R) and the values in the parentheses are the number of nodes that have the same name in each cluster. }}{29}{table.3}}
\bibstyle{plainnat}
\bibdata{tensor_wang}
\bibcite{acar2010scalable}{{1}{2010}{{Acar et~al.}}{{}}}
\bibcite{baltrunas2011incarmusic}{{2}{2011}{{Baltrunas et~al.}}{{}}}
\bibcite{bhaskar2016probabilistic}{{3}{2016}{{Bhaskar}}{{}}}
\bibcite{bhaskar20151}{{4}{2015}{{Bhaskar and Javanmard}}{{}}}
\bibcite{brascamp2002extensions}{{5}{2002}{{Brascamp and Lieb}}{{}}}
\bibcite{cai2013max}{{6}{2013}{{Cai and Zhou}}{{}}}
\bibcite{carlson2012physiology}{{7}{2012}{{Carlson}}{{}}}
\bibcite{chen2019non}{{8}{2019}{{Chen et~al.}}{{}}}
\bibcite{davenport2014}{{9}{2014}{{Davenport et~al.}}{{}}}
\bibcite{de2008tensor}{{10}{2008}{{De~Silva and Lim}}{{}}}
\bibcite{friedland2018nuclear}{{11}{2018}{{Friedland and Lim}}{{}}}
\newlabel{figure:brain image}{{C.2}{30}{Clustering results on HCP}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces In the left brain image, yellow nodes are from cluster I and Green nodes are from cluster II. Nodes from each cluster from III to VIII have almost same locations. The right brain image shows the extent of connectivity among nodes in each cluster I and cluster II. We take average of connections among nodes over 136 individuals. Yellow color means strong connectivity and green color weak connectivity. }}{30}{figure.6}}
\bibcite{geddes2016human}{{12}{2016}{{Geddes}}{{}}}
\bibcite{ghadermarzy2018learning}{{13}{2018}{{Ghadermarzy et~al.}}{{}}}
\bibcite{ghadermarzy2019near}{{14}{2019}{{Ghadermarzy et~al.}}{{}}}
\bibcite{hitchcock1927expression}{{15}{1927}{{Hitchcock}}{{}}}
\bibcite{hong2018generalized}{{16}{2019}{{Hong et~al.}}{{}}}
\bibcite{hore2016tensor}{{17}{2016}{{Hore et~al.}}{{}}}
\bibcite{jiang2017tensor}{{18}{2017}{{Jiang et~al.}}{{}}}
\bibcite{kolda2009tensor}{{19}{2009}{{Kolda and Bader}}{{}}}
\bibcite{lim2005singular}{{20}{2005}{{Lim}}{{}}}
\bibcite{mccullagh1980regression}{{21}{1980}{{McCullagh}}{{}}}
\bibcite{mu2014square}{{22}{2014}{{Mu et~al.}}{{}}}
\bibcite{negahban2011estimation}{{23}{2011}{{Negahban et~al.}}{{}}}
\bibcite{nguyen2015tensor}{{24}{2015}{{Nguyen et~al.}}{{}}}
\bibcite{nickel2011three}{{25}{2011}{{Nickel et~al.}}{{}}}
\bibcite{oseledets2011tensor}{{26}{2011}{{Oseledets}}{{}}}
\bibcite{reed1994nature}{{27}{1994}{{Reed and Caselli}}{{}}}
\bibcite{sur2019modern}{{28}{2019}{{Sur and Cand{\`e}s}}{{}}}
\bibcite{tomioka2014spectral}{{29}{2014}{{Tomioka and Suzuki}}{{}}}
\bibcite{tsybakov2009introduction}{{30}{2009}{{Tsybakov}}{{}}}
\bibcite{tucker1966some}{{31}{1966}{{Tucker}}{{}}}
\bibcite{wang2017operator}{{32}{2017}{{Wang et~al.}}{{}}}
\bibcite{wang2018learning}{{33}{2018}{{Wang and Li}}{{}}}
\bibcite{zeng2019multiway}{{34}{2019}{{Wang and Zeng}}{{}}}
\bibcite{xia2019sup}{{35}{2019}{{Xia and Zhou}}{{}}}
\bibcite{yuan2016tensor}{{36}{2016}{{Yuan and Zhang}}{{}}}
\bibcite{zhang2019cross}{{37}{2019}{{Zhang et~al.}}{{}}}
\bibcite{zhou2013tensor}{{38}{2013}{{Zhou et~al.}}{{}}}
