%%%%%%%% ICML 2020 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2020}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2020}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Ordinal tensor denoising and completion}

\usepackage{multirow}
\usepackage{graphicx}
%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath,amssymb}
\usepackage{amsthm}    % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{bm}
%\usepackage{subfig}
%\usepackage[english]{babel}
%\usepackage{algorithm}
%\usepackage{appendix}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}

\usepackage{dsfont}
%\usepackage{algpseudocode,algorithm}
%\algnewcommand\algorithmicinput{\textbf{Input:}}
%\algnewcommand\algorithmicoutput{\textbf{Output:}}
%\algnewcommand\INPUT{\item[\algorithmicinput]}
%\algnewcommand\OUTPUT{\item[\algorithmicoutput]}
%\DeclareMathOperator*{\minimize}{minimize}




\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}

\input macros.tex

\begin{document}

\twocolumn[
\icmltitle{Tensor denoising and completion based on ordinal observations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2020
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chanwoo Lee}{equal,to}
\icmlauthor{Miaoyan Wang}{equal,to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Statistics, University of Wisconsin at Madison, USA}

\icmlcorrespondingauthor{Miaoyan Wang}{miaoyan.wang@wisc.edu.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Higher-order tensors, ordinal data, tensor denoising, tensor completion}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}


\end{abstract}


\section{Introduction}
Multidimensional array data, a.k.a. a tensor, appears in a huge variety of applications including recommendation systems \citep{Kutty2012APR,Adomavicius2008ContextawareRS,Sun2015ProvableST},
social networks \citep{Sun2009MultiVisCS,Nickel2011ATM},
genomics \citep{Wang2017ThreewayCO},
neuroimaging (EEG, fMRI) \citep{Miwakeichi_decomposingeeg}
and signalprocessing \citep{Sidiropoulos2000ParallelFA,Cichocki2015TensorDF}.
Instead of unfolding those data tensors into matrices where many analysis methods have been proposed, we preserve its inherent multi-modal structure. Studying tensor data while respecting the structure allows us to examine complex interactions among tensor entries. Thereby we can provide extra more interpretation that cannot be addressed by traditional matrix analysis. Also, It has been shown that the tensor preserving analysis improves performance \citep{Zare2018ExtensionOP,Wang2018LearningFB}.
With those reasons, there is a growing need to develop dimension reduction method without losing multi modal structure. In the line of the attempts, a number of tensor decomposition methods have been proposed in many applications. CANDECOMP/PARAFAC (CP) decomposition was first introduced \citep{doi:10.1002/sapm19287139}
and revitalized in psychometrics \citep{Harshman1970FoundationsOT} and in linguistics \citep{Smilde2004MultiwayAW}. The Tucker decomposition was proposed in psychometrics \citep{tucker64extension,RePEc:spr:psycho:v:31:y:1966:i:3:p:279-311}.

Classical tensor completion with those decompositions has treated the entries of data as real-valued.
In many cases, However, we encounter data of which the entries are not real-valued but discrete or quantized i.e. binary-valued or multiple-valued. For example, many survey data takes the integer values. To be specific, the data in the Netflix problem has the 3 modes of `user',`movie' and `date of grade'. The entries of the data are the gradings from the users which take integer value from 1 to 5. Also, there are many cases that the data are quantized in real application. In signal processing, the data are frequently rounded or truncated so that only integer values are available. Also, in graph theory, adjacency matrix can be labeled as from 1 to 3 taking 3 when pairs of vertexes have strong connection and giving 1 when two vertexes have the weak connection according to a given threshold. If we add one more mode on adjacency matrix such as `context' or `individual', the data turns into tensor data with 3 integer values.

Therefore, performance improvement can be achieved when the observations are treated as discrete value not as continuous value.
In matrix case, there has been many achievements to complete matrix for discrete cases: Models for the case of binary or 1-bit were introduced and studied \citep{Davenport20121BitMC,Bhaskar20151bitMC}. Furthermore,  \citet{Bhaskar2016ProbabilisticLM} suggested matrix completion method for general ordinal observations.
In tensor case, however, only binary tensor has gotten an attention and achieved performance improvement using binary tensor decomposition methods \citep{Hore2016TensorDF,Wang2018LearningFB,Hong2018GeneralizedCP,Hu2018TrainingBW}. Accordingly, a general method for the data which has more than 2 ordered label is needed.

We organized this paper as follows. In section 2, we discuss detailed assumptions and descriptions about our probabilistic model in \eqref{multinomial}. Also, we suggest the estimation method for the latent parameters and related algorithm. In section 3, we provide the statistical properties of the upper, lower bounds and the phase-transition. We then provide the numerical experiments. Our model is applied to  real-world data to check validity and performance in section 5. Finally, we wrap up the paper with a discussion.

\section{Preliminaries}
Let $\tY\in\mathbb{R}^{d_1\times \cdots \times d_K}$ denote an order-$K$ $(d_1,\ldots,d_K)$-dimensional tensor. We use $y_\omega$ to denote the tensor entry indexed by $\omega$, where $\omega\in[d_1]\times\cdots\times[d_K]$.  The Frobenius norm of $\tY$ is defined as $\FnormSize{}{\tY}=\sum_\omega y^2_\omega$ and the infinity norm of $\tY$ is defined as $\mnormSize{}{\tY}=\max_{\omega}|y_\omega|$. We use $\tY_{(k)}$ to denote the unfolded matrix of size $d_k$-by-$\prod_{i\neq k}d_k$, obtained by reshaping the tensor along the mode-$k$. The Tucker rank of $\tY$ is defined as a length-$K$ vector $\mr = (r_1,\ldots,r_K)$, where $r_k$ is the rank of matrix $\tY_{(k)}$ for all $k \in[L]$. We say that an event $A$ occurs ``with very high probability'' if $\mathbb{P}(A)$ tends to 1 faster than any polynomial of tensor dimension $d_{\min}=\min\{d_1,\ldots,d_K\}$ as $d_{\min}\to\infty$.

We use lower-case letters (e.g., $a$, $b$, $c$) for scalars/vectors, upper-case boldface letters (e.g., $\mA$, $\mB$, $\mC$) for matrices, and calligraphy letters (e.g., $\tA$, $\tB$, $\tC$) for tensors of order three or greater. For ease of notation, we allow the basic arithmetic operators (e.g., $\leq, +, -$) to be applied to pairs of vectors in an element-wise manner. We use the shorthand $[n]$ to denote the $n$-set $\{1,\ldots,n\}$ for $n \in N_{+}$.



\section{Model}
\subsection{Low-rank cumulative model}
For the K-mode ordinal tensor $\mathcal{Y} = [\![ y_\omega]\!]\in[L]^{d_1\times\cdots\times d_K}$, we assume that its entries are realizations of independent multinomial random variables, such that
\begin{equation}
    \begin{aligned}
        \label{mainmodel}
        \mathbb{P}(y_\omega=l) = f_l(\theta_\omega), \quad \omega\in [d_1]\times\cdots\times[d_K].
    \end{aligned}
\end{equation}
In this model, a twice differentiable function $f_l : \mathbb{R}\rightarrow [0,1]$ with $l\in [L]$ is strictly increasing and satisfies $\sum_l f_l(\theta) = 1$ for a fixed $\theta$.
The tensor $\Theta = [\![\theta_\omega]\!]\in \mathbb{R}^{d_1\times\cdots\times d_K}$
is a hidden parameter which we are interested in. We assume the parameter tensor $\Theta$ is continuous value and admits a rank $\mr = (r_1,\cdots,r_K)$ Tucker decomposition,
\begin{equation}
    \begin{aligned}
        \label{tucker}
        \Theta = \mathcal{C}\times_1\mM_1\cdots\times_K\mM_N,
    \end{aligned}
\end{equation}
where $\mathcal{C}\in \mathbb{R}^{r_1\times\cdots\times r_K}$ is a core tensor and $\mM_k\in \mathbb{R}^{d_k\times r_k}$, for $n \in [L]$ is a factor matrix. We can get information about the influence of each mode by checking factor matrices.

The tensor model \eqref{mainmodel} has an equivalent representation as a latent model with K-level quantization. \citep{Davenport20121BitMC,Lan2014MatrixRF,Bhaskar20151bitMC,Cai2013AMC} where
$\mathcal{Y} = [\![ y_\omega]\!]$ is a quantized value such that,
\begin{equation}
    \label{latentmd}
\begin{aligned}
    y_\omega = \mathcal{Q}(\theta_\omega+\epsilon_\omega),\quad \omega\in [d_1]\times\cdots[d_K],
\end{aligned}
\end{equation}
where $\mathcal{E} = [\![\epsilon_\omega]\!]$ is a noise tensor of $i.i.d.$ from cumulative distribution function $\Phi(\cdot)$ and the function $\mathcal{Q}:\mathbb{R}\rightarrow [L]$ is a quantizer having the following rule.
\begin{equation}
    \label{qfunction}
    \begin{aligned}
        \mathcal{Q}(x) = l,\quad \text{ if } b_{l-1}< x \leq b_{l},\quad l\in [L],
    \end{aligned}
\end{equation}
where $\mb = (b_1,\cdots,b_{L-1})$ is a cutpoints vector such that $ -\infty = b_0<b_1<\cdots<b_L = \infty$. That is, the entries of observed tensor $\mathcal{Y}$ fall in category $l$ when the associated entries of the latent tensor $\Theta+\mathcal{E}$ fall in the $l$th interval of values. Then we have
\begin{equation}
    \label{cumulative}
    \begin{aligned}
        f_l(\theta_\omega) &= \mathbb{P}(y_\omega = l)\\
        &= \Phi(b_l-\theta_\omega)-\Phi(b_{l-1}-\theta_\omega).
    \end{aligned}
\end{equation}
We can diversify our model by the choices of $\Phi(\cdot)$, or equivalently the distribution of $\mathcal{E}$ with given $\mb$.
The followings are 2 common choices of $\Phi(\cdot)$.
\begin{example}[Logistic link/Logistic noise] The logistic model is represented by \eqref{mainmodel} with $f_l(\theta) = \Phi_{log}(\frac{b_l-\theta}{\sigma})-\Phi_{log}(\frac{b_{l-1}-\theta}{\sigma})$  where $\Phi_{log}(x/\sigma) = (1+e^{-x/\sigma})$. Equivalently, the noise $\epsilon_\omega$ in \eqref{latentmd} follows i.i.d. logistic distribution with the scale parameter $\sigma.$
\end{example}

\begin{example}[Probit link/Gaussian noise] The probit model is represented by \eqref{mainmodel} with $f_l(\theta) = \Phi_{norm}(\frac{b_l-\theta}{\sigma})-\Phi_{norm}(\frac{b_{l-1}-\theta}{\sigma})$ where $\Phi_{norm}$ is the cumulative distribution function of the standard Gaussian. Equivalently, the noise $\epsilon_\omega$ in \eqref{latentmd} follows i.i.d.$N(0,\sigma^2)$.
\end{example}

\subsection{Rank-constrained likelihood-based estimation}
Our goal is to estimate unknown parameter tensor $\Theta$ and a cutpoints vector $\mb$ from observed tensor $\mathcal{Y}$ using a constrained likelihood approach. With a little abuse of notation, we use $\Omega$ to denote either the full index set $\Omega = [d_1]\times\cdots\times[d_K]$ or a random subset indeuced from the subsampling distribution. The log-likelihood function for \eqref{mainmodel} is
\begin{equation}
    \label{likelihood}
    \begin{aligned}
    \mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb) = \sum_{\omega\in\Omega}\Big[\sum_{l\in[L]}\log(f_l(\theta_\omega))\mathds{1}_{\{y_\omega = l\}}\Big],
    \end{aligned}
\end{equation}
where the cutpoints vector $\mb$ is implicitly contained in the function $f_l$.
Considering the Tucker structure in \eqref{tucker}, we have the following constrained optimization problem.
\begin{equation}
    \label{eq:optimization}
    \begin{aligned}
        &\max_{(\Theta, \mb)\in \mathcal{D}\times\mathcal{B}}\mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb),\text{ where }\\
        &\mathcal{D} = \{\Theta\in \mathbb{R}^{d_1\times\cdots\times d_K}: \rank(\Theta) = \bm{r} \text{ and } \mnormSize{}{\Theta}\leq \alpha \}\\
        &\mathcal{B} = \{\mb\in\mathbb{R}^{L-1}:  b_1<\cdots < b_{L-1}\},
    \end{aligned}
\end{equation}
for a given rank $\bm r\in \mathbb{N_+}^K$ and a bound $\alpha \in \mathbb{R}_+$. The search space $\mathcal{D}$ has two constraints on unknown parameter $\Theta$. The first constraint ensures that the unknown parameter $\Theta$ admits the Tucker decomposition with rank $\bm r$. The second constraint makes the entries of $\Theta$ bounded by a constant $\alpha$. This bound condition is a technical assumption to help to recover $\Theta$ in the noiseless case. Similar conditions has been imposed in many literatures for the matrix case \citep{Davenport20121BitMC,Bhaskar20151bitMC,Cai2013AMC,Bhaskar2016ProbabilisticLM}.
The search space $\mathcal{B}$ makes sure that the probability function $f_l$ in \eqref{cumulative} is positive.

\subsection{Optimization}
In this section, we describe the algorithm to seek the optimizer of \eqref{eq:optimization}. The objective function $\mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb)$ is concave in $(\Theta,\mb)$ whenever $\Phi(x)$ is log-concave \citep{McCullagh1980RegressionMF,Burridge1981ANO}.
However, the feasible set $\mathcal{D}$ is not a convex set, which makes the optimization \eqref{eq:optimization} a non-convex problem. One approach to handle this problem is utilizing the Tucker decomposition and converting optimization into a block-wise convex problem.
Let us denote the objective function as
\begin{equation}
    \label{blockwise}
    \begin{aligned}
         \mathcal{L}(\mathcal{C},\mM_1,\cdots,\mM_K,\mb) = \mathcal{L}_{\mathcal{Y},\Omega}(\mathcal{C}\times_1\mM_1\cdots\times_K\mM_K,\mb).
    \end{aligned}
\end{equation}
From \eqref{blockwise}, we have $K+2$ blocks of variables in the objective function, one for the cutpoints vector $\mb$, one for the core tensor $\mathcal{C}$ and  K for the factor matrices $\mM_K$'s.
We can change the optimization problem to simple convex problem if any $K+1$ out of the $K+2$ blocks being fixed. Therefore, we can alternatively update one block at a time while other blocks being fixed.
The algorithm \ref{alg} gives the full description.



\begin{algorithm}[tb]
        \caption{Ordinal tensor decomposition }\label{alg}
        \begin{algorithmic}[]
            \STATE{\bfseries Input:}  \text{ Ordinal tensor }
            $\mathcal{Y}\in [L]^{d_1\times\cdots\times d_K}
            $,\\ \hspace{.43in} Rank $\mr\in \mathbb{N_+}^{K-1}$,\\
            \hspace{.43in} Entry-wise bound $\alpha\in \mathbb{R_+}$.
            \STATE{\bfseries Output:} $(\hat\Theta,\hat{\mb}) =  \argmax_{(\Theta,\mb)\in \mathcal{D}\times\mathcal{B}}  \mathcal{L}_{\mathcal{Y},\Omega}(\Theta,\mb).$
            \STATE Initialize  Core tensor $\mathcal{C}^{(0)}$,\\
            \hspace{.5in} Factor matrices $ \{\mM_1^{(0)},\cdots,\mM_K^{(0)}\}$,\\
            \hspace{.5in} Cut-off points $\mb^{(0)}$.
            \FOR{t = 1,2,$\cdots$,}
            \FOR{k = 1,2,$\cdots$,K}
            \STATE{Update $\mM_k$ fixing other blocks.}
            \STATE $\mM_k^{(t+1)}\gets\argmax_{\mM_k\in\mathbb{R}^{d_k\times r_k}}\mathcal{L}_{\mathcal{Y},\Omega}(\mM_k)$ s.t.
             $\|\mathcal{C}^{(t)}\times_1\mM_1^{(t+1)}\cdots\times_k\mM_k\cdots\times_K\mM_K^{(t)}\|_\infty \leq \alpha$
            \ENDFOR
            \STATE {Update $\mathcal{C}$ fixing other blocks.}
            \STATE $\mathcal{C}^{(t+1)} \gets \argmax_{\mathcal{C}\in \mathbb{R}^{r_1\times\cdots\times r_k}}\mathcal{L}_{\mathcal{Y},\Omega}(\mathcal{C})$
            s.t.\newline $\|\mathcal{C}\times_1\mM_1^{(t+1)}\cdots\times_K\mM_K^{(t+1)}\|_\infty \leq \alpha$

            \STATE $\Theta^{(t+1)} \gets \mathcal{C}^{(t+1)}\times_1\mM_1^{(t+1)}\cdots\times_K\mM_K^{(t+1)}$
            \STATE {Update $\mb$ fixing $\Theta^{(t+1)}$.}
            \STATE $\mb^{(t+1)} \gets \argmax_{\mb\in \mathbb{R}^{L-1}}\mathcal{L}_{\mathcal{Y},\Omega}\big(\Theta^{(t+1)},\mb\big)$
            \ENDFOR
            \STATE \textbf{return}
            $\Theta,\mb$
        \end{algorithmic}
    \end{algorithm}

\subsection{Rank selection}
Algorithm \ref{alg} takes  the rank $\bm r$ as an input variable. In practice, the rank $\bm r$ is hardly known. Estimating an appropriate rank $\bm r$ from a given tensor is an important issue. We suggest to use Bayesian Information Criterion(BIC) to choose the rank.
\begin{equation}
    \label{bic}
    \begin{aligned}
        \hat{\bm r} &= \argmin_{\bm r\in \mathbb{N}_+^K} BIC(\bm r)\\
        &=\argmin_{\bm r\in \mathbb{N}_+^K}\bigg[-2\mathcal{L}_\mathcal{Y}(\hat\Theta(\bm r)) \quad+
        \\&\qquad\big(\prod_{k\in[K]} r_k+\sum_{k\in[K]}(d_kr_k-r_k^2)\big)\log(\prod_{k\in[K]} d_k)\bigg],
    \end{aligned}
\end{equation}
where $\hat\Theta(\bm r)$ is a maximum likelihood estimater given the rank $\bm r$. The second part of the summation in \eqref{bic} is the number of  independent parameters in the model. We select the rank $\hat{\bm r}$ that minimizes BIC value through the grid search method.
\section{Real-world Data Applications}
In this section, we apply our ordinal tensor decomposition method to two real-world datasets of ordinal tensors. In the first application, we use our model to analyze an ordinal tensor consisting of structural connectivity patterns among 68 brain regions for 136 individuals from Human Connectome Project (HCP) \citep{Geddes2016HumanBM}. In the second application, we perform tensor completion from the data with missing values. The data tensor records the ratings from scale 1 to 5 of 42 users to 139 songs on 26 contexts \citep{Baltrunas2011InCarMusicCM}.
\subsection{Human Connectome Project (HCP)}
The human connectome project (HCP) is a $68\times 68 \times 136$ tensor where the first two modes have 68 indices representing brain regions and the last mode has 136 indices meaning individuals. All the individual images were preprocessed following a standard pipeline \citep{Zhang2018MappingPS}, and the brain was parcellated to 68 regions of interest following the Desikan atlas \citep{Desikan2006AnAL}. The tensor entries consist of $\{1,2,3\}$, the strength of fiber connections between 68 brain regions for each of the 136 individuals. First, we apply our ordinal tensor decomposition method with a logistic link function to the HCP data and identify similar brain nodes based on latent parameters. The BIC result suggests $\bm r = (23,23,8)$  with $\mathcal{L}_\mathcal{Y}(\hat{\Theta},\hat{\mb}) = -216645.8$.



Also, we compare our ordinal tensor decomposition method with a logistic link function with other methods: the continuous tensor decomposition method, the 1 bit-tensor completion method with probit link function and the 1 bit-tensor completion method with logistic link function. We use 5 folded cross validation method for the comparison. Specifically, we randomly split the tensor entries into 5 similar sized pieces and alternatively use each piece of entries as a test data using other $80\%$ entries as a training data. The entries in the test data are encoded as missing and then predicted based on each method from the training data. We set the rank of the tensor as $\bm r = (23,23,8)$ for the tensor decomposition methods which minimizes BIC value. Table \ref{CV} shows RMSE, MAE and error(false prediction rate), averaged over 5 test set results. We can check that our ordinal tensor decomposition model outperforms the other methods in all criteria.

\begin{table}[t]
\label{CV}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
 Method & RMSE & MAE & ERROR \\
\midrule
\begin{tabular}[c]{@{}c@{}}Oridnal tensor\\ Decomposition\end{tabular}  & 0.1503711 & 0.1502662 & 0.1502151 \\ \\
\begin{tabular}[c]{@{}c@{}}Conti tensor\\ Decomposition\end{tabular} & 0.1603685& 0.1600219& 0.1598499\\ \\
\begin{tabular}[c]{@{}c@{}}1bit-completion\\ (probit)\end{tabular}  & 0.3658390 & 0.3632757 & 0.3619940\\
\begin{tabular}[c]{@{}c@{}}1bit-completion\\ (logistic)\end{tabular}  &  0.4519253 & 0.4218513 & 0.4068143 
\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Results of comparisons among 4 methods on the HCP data predicting the test data. Four methods are the ordinal tensor decomposition algorithm, the continuous tensor decomposition algorithm and the 1 bit tensor completion method with probit link function and logistic link function. Each method is evaluated by RMSE, MAE, error(false prediction rate).}
\end{table}

\subsection{Music data with missing values}

InCarMusic is a mobile application that offers music recommendation to passengers of cars based on contexts \citep{Baltrunas2011InCarMusicCM}.
Our goal is to get the tensor completion from the $42\times139\times 26$ tensor using the ordinal tensor decomposition and thereby we can offer context-specific music recommendation to users.
The tensor entries consist of $\{1,2,3,4,5\}$, the ratings of 42 users to 139 songs on 26 contexts and are encoded as $-1$ for missing values. The number of missing values is $148904$ and the number of available values is $2884$.

\bibliography{ICML.bib}
\bibliographystyle{icml2020}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2020. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
